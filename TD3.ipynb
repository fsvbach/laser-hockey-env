{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twin Delayed Deep Deterministic Policy Gradient (TD3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "from laserhockey.hockey_env import BasicOpponent\n",
    "from laserhockey.TrainingHall import TrainingHall\n",
    "import gym\n",
    "#import roboschool\n",
    "import sys\n",
    "from DDPG.ddpg_agent import DDPGAgent\n",
    "from DQN.agent import DQNAgent\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Initialize parameters and build model.\n",
    "        Args:\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            max_action (float): highest action to take\n",
    "            seed (int): Random seed\n",
    "            h1_units (int): Number of nodes in first hidden layer\n",
    "            h2_units (int): Number of nodes in second hidden layer\n",
    "            \n",
    "        Return:\n",
    "            action output of network with tanh activation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, action_dim)\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.max_action * torch.tanh(self.l3(x)) \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Initialize parameters and build model.\n",
    "        Args:\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            max_action (float): highest action to take\n",
    "            seed (int): Random seed\n",
    "            h1_units (int): Number of nodes in first hidden layer\n",
    "            h2_units (int): Number of nodes in second hidden layer\n",
    "            \n",
    "        Return:\n",
    "            value output of network \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.l4 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.l5 = nn.Linear(400, 300)\n",
    "        self.l6 = nn.Linear(300, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "\n",
    "        x1 = F.relu(self.l1(xu))\n",
    "        x1 = F.relu(self.l2(x1))\n",
    "        x1 = self.l3(x1)\n",
    "\n",
    "        x2 = F.relu(self.l4(xu))\n",
    "        x2 = F.relu(self.l5(x2))\n",
    "        x2 = self.l6(x2)\n",
    "        return x1, x2\n",
    "\n",
    "\n",
    "    def Q1(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "\n",
    "        x1 = F.relu(self.l1(xu))\n",
    "        x1 = F.relu(self.l2(x1))\n",
    "        x1 = self.l3(x1)\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code based on: \n",
    "# https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
    "\n",
    "# Expects tuples of (state, next_state, action, reward, done)\n",
    "class ReplayBuffer(object):\n",
    "    \"\"\"Buffer to store tuples of experience replay\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size=150000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_size (int): total amount of tuples to store\n",
    "        \"\"\"\n",
    "        \n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    def add(self, data):\n",
    "        \"\"\"Add experience tuples to buffer\n",
    "        \n",
    "        Args:\n",
    "            data (tuple): experience replay tuple\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = data\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Samples a random amount of experiences from buffer of batch size\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): size of sample\n",
    "        \"\"\"\n",
    "        \n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        states, actions, next_states, rewards, dones = [], [], [], [], []\n",
    "\n",
    "        for i in ind: \n",
    "            s, a, s_, r, d = self.storage[i]\n",
    "            states.append(np.array(s, copy=False))\n",
    "            actions.append(np.array(a, copy=False))\n",
    "            next_states.append(np.array(s_, copy=False))\n",
    "            rewards.append(np.array(r, copy=False))\n",
    "            dones.append(np.array(d, copy=False))\n",
    "\n",
    "        return np.array(states), np.array(actions), np.array(next_states), np.array(rewards).reshape(-1, 1), np.array(dones).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(object):\n",
    "    \"\"\"Agent class that handles the training of the networks and provides outputs as actions\n",
    "    \n",
    "        Args:\n",
    "            state_dim (int): state size\n",
    "            action_dim (int): action size\n",
    "            max_action (float): highest action to take\n",
    "            device (device): cuda or cpu to process tensors\n",
    "            env (env): gym environment to use\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, observation_space, action_space, pretrained=False):\n",
    "        state_dim  = observation_space.shape[0]\n",
    "        action_dim = action_space.shape[0]\n",
    "        max_action = float(action_space.high[0])\n",
    "        \n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-3)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        #self.scaling = torch.tensor([ 5.0,  5.0 , 5.0, 3.0, 3.0, 3.0,  \n",
    "        #                                2.0,  2.0,  1.0, 1.0, 1.0, 1.0,  \n",
    "        #                               10.0, 10.0, 10.0, 10.0, 5.0 ,5.0])\n",
    "        if pretrained:\n",
    "            self.load(pretrained)\n",
    "\n",
    "\n",
    "        \n",
    "    def act(self, state, eps=0.0):\n",
    "        \"\"\"Select an appropriate action from the agent policy\n",
    "        \n",
    "            Args:\n",
    "                state (array): current state of environment\n",
    "                eps (float): how much noise to add to acitons\n",
    "                \n",
    "            Returns:\n",
    "                action (float): action clipped within action range\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)#*self.scaling\n",
    "        action = self.actor(state).cpu().data.numpy().flatten()\n",
    "        if eps != 0: \n",
    "            action = (action + np.random.normal(0, eps, size=self.action_space.shape[0]))\n",
    "            \n",
    "        return action.clip(self.action_space.low, self.action_space.high)\n",
    "\n",
    "    \n",
    "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "        \"\"\"Train and update actor and critic networks\n",
    "        \n",
    "            Args:\n",
    "                replay_buffer (ReplayBuffer): buffer for experience replay\n",
    "                iterations (int): how many times to run training\n",
    "                batch_size(int): batch size to sample from replay buffer\n",
    "                discount (float): discount factor\n",
    "                tau (float): soft update for main networks to target networks\n",
    "                \n",
    "            Return:\n",
    "                actor_loss (float): loss from actor network\n",
    "                critic_loss (float): loss from critic network\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        for it in range(iterations):\n",
    "\n",
    "            # Sample replay buffer \n",
    "            x, y, u, r, d = replay_buffer.sample(batch_size)\n",
    "            state = torch.FloatTensor(x).to(device)#*self.scaling\n",
    "            action = torch.FloatTensor(u).to(device)\n",
    "            next_state = torch.FloatTensor(y).to(device)#*self.scaling\n",
    "            done = torch.FloatTensor(1 - d).to(device)\n",
    "            reward = torch.FloatTensor(r).to(device)\n",
    "            \n",
    "            # #add noise to observation\n",
    "            #clean_idx = (torch.tensor([ 0,1,2,3,4,5,12,13,14,15, 16, 17]),)\n",
    "            #noise = torch.FloatTensor(state).data.normal_(0, 1).to(device)\n",
    "            #noise[clean_idx] = 0\n",
    "            #state += noise\n",
    "\n",
    "            # Select action according to policy and add clipped noise \n",
    "            noise = torch.FloatTensor(u).data.normal_(0, policy_noise).to(device)\n",
    "            noise = noise.clamp(-noise_clip, noise_clip)\n",
    "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            # Compute the target Q value\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + (done * discount * target_Q).detach()\n",
    "\n",
    "            # Get current Q estimates\n",
    "            current_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "            # Compute critic loss\n",
    "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q) \n",
    "\n",
    "            # Optimize the critic\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            # Delayed policy updates\n",
    "            if it % policy_freq == 0:\n",
    "\n",
    "                # Compute actor loss\n",
    "                actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "\n",
    "                # Optimize the actor \n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                # Update the frozen target models\n",
    "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "\n",
    "    def save(self, filename, directory):\n",
    "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "\n",
    "\n",
    "    def load(self, filename=\"best_avg\", directory=\"TD3/saves\"):\n",
    "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner():\n",
    "    \"\"\"Carries out the environment steps and adds experiences to memory\"\"\"\n",
    "    \n",
    "    def __init__(self, env, agent, replay_buffer):\n",
    "        \n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.obs = env.reset()\n",
    "        self.done = False\n",
    "        \n",
    "    def next_step(self, episode_timesteps, noise=0.1):\n",
    "        \n",
    "        action = self.agent.act(np.array(self.obs), eps=0.1)\n",
    "\n",
    "        # Perform action\n",
    "        new_obs, reward, done, info = self.env.step(action) \n",
    "        done_bool = 0 if episode_timesteps + 1 == 255 else float(done)\n",
    "        \n",
    "        #add proxy\n",
    "        reward += sum(list(info.values()))\n",
    "        \n",
    "        # Store data in replay buffer\n",
    "        replay_buffer.add((self.obs, new_obs, action, reward, done_bool))\n",
    "        \n",
    "        self.obs = new_obs\n",
    "        \n",
    "        if done:\n",
    "            self.obs = self.env.reset()\n",
    "            done = False\n",
    "            \n",
    "            return reward, True\n",
    "        \n",
    "        return reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, env, eval_episodes=100,render=False):\n",
    "    \"\"\"run several episodes using the best agent policy\n",
    "        \n",
    "        Args:\n",
    "            policy (agent): agent to evaluate\n",
    "            env (env): gym environment\n",
    "            eval_episodes (int): how many test episodes to run\n",
    "            render (bool): show training\n",
    "        \n",
    "        Returns:\n",
    "            avg_reward (float): average reward over the number of evaluations\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    avg_reward = 0.\n",
    "    for i in range(eval_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = policy.act(np.array(obs), eps=0)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "\n",
    "    print(\"\\n---------------------------------------\")\n",
    "    print(\"Evaluation over {:d} episodes: {:f}\" .format(eval_episodes, avg_reward))\n",
    "    print(\"---------------------------------------\")\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe(env,replay_buffer, observation_steps):\n",
    "    \"\"\"run episodes while taking random actions and filling replay_buffer\n",
    "    \n",
    "        Args:\n",
    "            env (env): gym environment\n",
    "            replay_buffer(ReplayBuffer): buffer to store experience replay\n",
    "            observation_steps (int): how many steps to observe for\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    time_steps = 0\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while time_steps < observation_steps:\n",
    "        action = env.action_space.sample()\n",
    "        new_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        replay_buffer.add((obs, new_obs, action, reward, done))\n",
    "\n",
    "        obs = new_obs\n",
    "        time_steps += 1\n",
    "\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "\n",
    "        print(\"\\rPopulating Buffer {}/{}.\".format(time_steps, observation_steps), end=\"\")\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env):\n",
    "    \"\"\"Train the agent for exploration steps\n",
    "    \n",
    "        Args:\n",
    "            agent (Agent): agent to use\n",
    "            env (environment): gym environment\n",
    "            writer (SummaryWriter): tensorboard writer\n",
    "            exploration (int): how many training steps to run\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    total_timesteps = 0\n",
    "    timesteps_since_eval = 0\n",
    "    episode_num = 0\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    done = False \n",
    "    evaluations = []\n",
    "    rewards = []\n",
    "    best_avg = -5\n",
    "    \n",
    "    writer = SummaryWriter(comment=\"-last\")\n",
    "    \n",
    "    while total_timesteps < EXPLORATION:\n",
    "    \n",
    "        if done and total_timesteps != 0:\n",
    "\n",
    "            rewards.append(episode_reward)\n",
    "            avg_reward = np.mean(rewards[-100:])\n",
    "\n",
    "            writer.add_scalar(\"avg_reward\", avg_reward, total_timesteps)\n",
    "            writer.add_scalar(\"reward_step\", reward, total_timesteps)\n",
    "            writer.add_scalar(\"episode_reward\", episode_reward, total_timesteps)\n",
    "\n",
    "            if best_avg < avg_reward:\n",
    "                best_avg = avg_reward\n",
    "                print(\"saving best model....\\n\")\n",
    "                agent.save(\"best_avg\",\"TD3/saves\")\n",
    "\n",
    "            print(\"\\rTotal T: {:d} Episode Num: {:d} Reward: {:f} Avg Reward: {:f}\".format(\n",
    "                total_timesteps, episode_num, episode_reward, avg_reward), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "\n",
    "            if avg_reward >= REWARD_THRESH:\n",
    "                break\n",
    "\n",
    "            agent.train(replay_buffer, \n",
    "                        episode_timesteps, \n",
    "                        BATCH_SIZE, GAMMA, TAU, NOISE, NOISE_CLIP, POLICY_FREQUENCY)\n",
    "\n",
    "            episode_reward = 0\n",
    "            episode_timesteps = 0\n",
    "            episode_num += 1 \n",
    "\n",
    "        reward, done = runner.next_step(episode_timesteps)\n",
    "        episode_reward += reward\n",
    "\n",
    "        episode_timesteps += 1\n",
    "        total_timesteps += 1\n",
    "        timesteps_since_eval += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SEED = 5\n",
    "OBSERVATION = 10000\n",
    "EXPLORATION = 1000000\n",
    "BATCH_SIZE = 100\n",
    "GAMMA = 0.99\n",
    "TAU = 0.005\n",
    "NOISE = 0.2\n",
    "NOISE_CLIP = 0.5\n",
    "EXPLORE_NOISE = 0.2\n",
    "POLICY_FREQUENCY = 2\n",
    "EVAL_FREQUENCY = 5000\n",
    "REWARD_THRESH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "current winratios: [0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = TrainingHall()\n",
    "\n",
    "basic  = BasicOpponent(weak=False)\n",
    "td3   = TD3(env.observation_space, env.action_space, pretrained='stronger')\n",
    "loser = TD3(env.observation_space, env.action_space, pretrained='smallhall')\n",
    "ddpg  = DDPGAgent(env,\n",
    "                actor_lr=1e-4,\n",
    "                critic_lr=1e-3,\n",
    "                update_rate=0.05,\n",
    "                discount=0.9, update_target_every=20,\n",
    "                pretrained='DDPG/weights/ddpg-normal-eps-noise-basic-35000')\n",
    "q_agent = DQNAgent(env.observation_space, \n",
    "                         env.discrete_action_space,\n",
    "                        convert_func =  env.discrete_to_continous_action,\n",
    "                        pretrained   = 'DQN/weights/training_hall_1')\n",
    "\n",
    "env.register_opponents([basic])#,ddpg,q_agent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "env.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "policy = TD3(env.observation_space, env.action_space, pretrained=False)\n",
    "\n",
    "replay_buffer = ReplayBuffer(max_size=150000)\n",
    "\n",
    "runner = Runner(env, policy, replay_buffer)\n",
    "\n",
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating Buffer 10000/10000."
     ]
    }
   ],
   "source": [
    "# Populate replay buffer\n",
    "observe(env, replay_buffer, OBSERVATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<laserhockey.hockey_env.BasicOpponent object at 0x7ff2745005e0>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 5277 Episode Num: 29 Reward: -14.640170 Avg Reward: -4.175097\n",
      "current winratios: [-0.44]\n",
      "\n",
      "Total T: 22631 Episode Num: 129 Reward: -17.535172 Avg Reward: -4.617820\n",
      "current winratios: [-0.4]\n",
      "\n",
      "Total T: 40097 Episode Num: 229 Reward: -18.537755 Avg Reward: -4.275965\n",
      "current winratios: [-0.38]\n",
      "\n",
      "Total T: 55168 Episode Num: 319 Reward: 6.890971 Avg Reward: -0.05422239saving best model....\n",
      "\n",
      "Total T: 55244 Episode Num: 320 Reward: 112.643019 Avg Reward: 0.934987saving best model....\n",
      "\n",
      "Total T: 55271 Episode Num: 321 Reward: 111.504060 Avg Reward: 2.209115saving best model....\n",
      "\n",
      "Total T: 55293 Episode Num: 322 Reward: -1.378624 Avg Reward: 2.212305saving best model....\n",
      "\n",
      "Total T: 55365 Episode Num: 323 Reward: 9.744542 Avg Reward: 2.485533saving best model....\n",
      "\n",
      "Total T: 55452 Episode Num: 324 Reward: 9.688353 Avg Reward: 2.595965saving best model....\n",
      "\n",
      "Total T: 55674 Episode Num: 325 Reward: 10.389505 Avg Reward: 2.871230saving best model....\n",
      "\n",
      "Total T: 55706 Episode Num: 326 Reward: -1.259754 Avg Reward: 2.873412saving best model....\n",
      "\n",
      "Total T: 55957 Episode Num: 327 Reward: -8.500676 Avg Reward: 2.975524saving best model....\n",
      "\n",
      "Total T: 56113 Episode Num: 328 Reward: 9.465865 Avg Reward: 3.037204saving best model....\n",
      "\n",
      "Total T: 56364 Episode Num: 329 Reward: 32.404788 Avg Reward: 3.546629\n",
      "current winratios: [-0.45]\n",
      "\n",
      "Total T: 56410 Episode Num: 330 Reward: -1.370734 Avg Reward: 3.174366saving best model....\n",
      "\n",
      "Total T: 56554 Episode Num: 331 Reward: 20.391109 Avg Reward: 3.552361saving best model....\n",
      "\n",
      "Total T: 56587 Episode Num: 332 Reward: -1.408573 Avg Reward: 3.553833saving best model....\n",
      "\n",
      "Total T: 56785 Episode Num: 333 Reward: 136.447246 Avg Reward: 5.100400saving best model....\n",
      "\n",
      "Total T: 56817 Episode Num: 334 Reward: -1.359561 Avg Reward: 5.103441saving best model....\n",
      "\n",
      "Total T: 56984 Episode Num: 335 Reward: 27.492219 Avg Reward: 5.540331saving best model....\n",
      "\n",
      "Total T: 57079 Episode Num: 336 Reward: 9.196147 Avg Reward: 5.649673saving best model....\n",
      "\n",
      "Total T: 57301 Episode Num: 338 Reward: -8.069394 Avg Reward: 5.793581saving best model....\n",
      "\n",
      "Total T: 57871 Episode Num: 342 Reward: -1.274962 Avg Reward: 5.625942saving best model....\n",
      "\n",
      "Total T: 58122 Episode Num: 343 Reward: 8.255137 Avg Reward: 5.871536saving best model....\n",
      "\n",
      "Total T: 58373 Episode Num: 344 Reward: 14.584430 Avg Reward: 6.034831saving best model....\n",
      "\n",
      "Total T: 58420 Episode Num: 345 Reward: 113.201749 Avg Reward: 7.341787saving best model....\n",
      "\n",
      "Total T: 58475 Episode Num: 346 Reward: -1.276164 Avg Reward: 7.343470saving best model....\n",
      "\n",
      "Total T: 58623 Episode Num: 347 Reward: 12.103339 Avg Reward: 7.654189saving best model....\n",
      "\n",
      "Total T: 58874 Episode Num: 348 Reward: 9.361025 Avg Reward: 7.763345saving best model....\n",
      "\n",
      "Total T: 59177 Episode Num: 350 Reward: -1.602588 Avg Reward: 7.887626saving best model....\n",
      "\n",
      "Total T: 59231 Episode Num: 351 Reward: 5.712904 Avg Reward: 8.092153saving best model....\n",
      "\n",
      "Total T: 59482 Episode Num: 352 Reward: 20.665505 Avg Reward: 8.313351saving best model....\n",
      "\n",
      "Total T: 59565 Episode Num: 353 Reward: 6.801505 Avg Reward: 8.538310saving best model....\n",
      "\n",
      "Total T: 59617 Episode Num: 354 Reward: -1.490935 Avg Reward: 8.539699saving best model....\n",
      "\n",
      "Total T: 59782 Episode Num: 355 Reward: 17.759436 Avg Reward: 8.834126saving best model....\n",
      "\n",
      "Total T: 60003 Episode Num: 356 Reward: 117.323145 Avg Reward: 9.778596saving best model....\n",
      "\n",
      "Total T: 60887 Episode Num: 361 Reward: -7.705425 Avg Reward: 9.871902saving best model....\n",
      "\n",
      "Total T: 61138 Episode Num: 362 Reward: 21.847952 Avg Reward: 10.010607saving best model....\n",
      "\n",
      "Total T: 61254 Episode Num: 363 Reward: 12.510641 Avg Reward: 10.238493saving best model....\n",
      "\n",
      "Total T: 64459 Episode Num: 384 Reward: -1.262369 Avg Reward: 10.2039797saving best model....\n",
      "\n",
      "Total T: 64541 Episode Num: 385 Reward: 8.371969 Avg Reward: 10.465201saving best model....\n",
      "\n",
      "Total T: 64661 Episode Num: 386 Reward: 13.071081 Avg Reward: 10.608086saving best model....\n",
      "\n",
      "Total T: 64854 Episode Num: 388 Reward: -1.433667 Avg Reward: 10.947666saving best model....\n",
      "\n",
      "Total T: 65105 Episode Num: 389 Reward: 7.767293 Avg Reward: 11.196249saving best model....\n",
      "\n",
      "Total T: 65200 Episode Num: 390 Reward: 123.588604 Avg Reward: 12.445823saving best model....\n",
      "\n",
      "Total T: 65451 Episode Num: 391 Reward: -6.004354 Avg Reward: 12.573218saving best model....\n",
      "\n",
      "Total T: 66022 Episode Num: 394 Reward: 26.386064 Avg Reward: 12.550590saving best model....\n",
      "\n",
      "Total T: 66273 Episode Num: 395 Reward: 31.891047 Avg Reward: 12.995863saving best model....\n",
      "\n",
      "Total T: 66346 Episode Num: 396 Reward: 4.840678 Avg Reward: 13.061519saving best model....\n",
      "\n",
      "Total T: 66460 Episode Num: 397 Reward: 18.360181 Avg Reward: 13.405543saving best model....\n",
      "\n",
      "Total T: 66711 Episode Num: 398 Reward: 24.746986 Avg Reward: 13.666554saving best model....\n",
      "\n",
      "Total T: 67177 Episode Num: 401 Reward: 23.887808 Avg Reward: 13.923089saving best model....\n",
      "\n",
      "Total T: 67275 Episode Num: 402 Reward: 19.166332 Avg Reward: 14.129696saving best model....\n",
      "\n",
      "Total T: 67373 Episode Num: 403 Reward: 12.924869 Avg Reward: 14.346161saving best model....\n",
      "\n",
      "Total T: 67429 Episode Num: 404 Reward: -1.566257 Avg Reward: 14.347231saving best model....\n",
      "\n",
      "Total T: 67485 Episode Num: 405 Reward: 111.962241 Avg Reward: 15.560717saving best model....\n",
      "\n",
      "Total T: 67736 Episode Num: 406 Reward: 20.253516 Avg Reward: 15.673383saving best model....\n",
      "\n",
      "Total T: 67963 Episode Num: 407 Reward: 134.545553 Avg Reward: 16.846686saving best model....\n",
      "\n",
      "Total T: 68214 Episode Num: 408 Reward: 7.783045 Avg Reward: 16.889531saving best model....\n",
      "\n",
      "Total T: 68610 Episode Num: 411 Reward: 11.144312 Avg Reward: 16.857909saving best model....\n",
      "\n",
      "Total T: 68790 Episode Num: 412 Reward: 117.403273 Avg Reward: 18.049264saving best model....\n",
      "\n",
      "Total T: 68878 Episode Num: 413 Reward: 7.773170 Avg Reward: 18.231322saving best model....\n",
      "\n",
      "Total T: 68964 Episode Num: 414 Reward: 3.711698 Avg Reward: 18.282054saving best model....\n",
      "\n",
      "Total T: 69096 Episode Num: 416 Reward: -1.448411 Avg Reward: 18.515977saving best model....\n",
      "\n",
      "Total T: 69248 Episode Num: 418 Reward: -1.355477 Avg Reward: 18.687875saving best model....\n",
      "\n",
      "Total T: 70574 Episode Num: 429 Reward: 10.813378 Avg Reward: 17.0964751\n",
      "current winratios: [-0.49]\n",
      "\n",
      "Total T: 81663 Episode Num: 529 Reward: 119.653152 Avg Reward: 18.396688\n",
      "current winratios: [-0.53]\n",
      "\n",
      "Total T: 88907 Episode Num: 577 Reward: 112.746337 Avg Reward: 19.742220saving best model....\n",
      "\n",
      "Total T: 89334 Episode Num: 581 Reward: 14.996256 Avg Reward: 20.8188377saving best model....\n",
      "\n",
      "Total T: 89530 Episode Num: 582 Reward: 38.835059 Avg Reward: 21.127763saving best model....\n",
      "\n",
      "Total T: 89626 Episode Num: 583 Reward: 9.433409 Avg Reward: 21.164016saving best model....\n",
      "\n",
      "Total T: 89705 Episode Num: 584 Reward: 13.646015 Avg Reward: 21.315955saving best model....\n",
      "\n",
      "Total T: 89854 Episode Num: 585 Reward: 41.409251 Avg Reward: 21.557188saving best model....\n",
      "\n",
      "Total T: 90008 Episode Num: 586 Reward: 15.733680 Avg Reward: 21.729839saving best model....\n",
      "\n",
      "Total T: 90129 Episode Num: 587 Reward: 9.153704 Avg Reward: 21.776561saving best model....\n",
      "\n",
      "Total T: 90444 Episode Num: 589 Reward: 111.206963 Avg Reward: 21.921556saving best model....\n",
      "\n",
      "Total T: 90617 Episode Num: 590 Reward: 24.042891 Avg Reward: 22.175687saving best model....\n",
      "\n",
      "Total T: 90647 Episode Num: 591 Reward: 112.369987 Avg Reward: 23.199106saving best model....\n",
      "\n",
      "Total T: 90728 Episode Num: 592 Reward: 5.318529 Avg Reward: 23.265130saving best model....\n",
      "\n",
      "Total T: 90881 Episode Num: 593 Reward: 36.018190 Avg Reward: 23.557274saving best model....\n",
      "\n",
      "Total T: 90985 Episode Num: 594 Reward: 16.784587 Avg Reward: 23.739820saving best model....\n",
      "\n",
      "Total T: 91055 Episode Num: 595 Reward: 112.710637 Avg Reward: 24.922105saving best model....\n",
      "\n",
      "Total T: 91417 Episode Num: 598 Reward: 46.855062 Avg Reward: 25.3098316saving best model....\n",
      "\n",
      "Total T: 91445 Episode Num: 599 Reward: 113.210637 Avg Reward: 26.388621saving best model....\n",
      "\n",
      "Total T: 91625 Episode Num: 600 Reward: 135.060541 Avg Reward: 27.752192saving best model....\n",
      "\n",
      "Total T: 91711 Episode Num: 601 Reward: 4.822575 Avg Reward: 27.861921saving best model....\n",
      "\n",
      "Total T: 94764 Episode Num: 618 Reward: 113.091175 Avg Reward: 27.840602saving best model....\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 95071 Episode Num: 621 Reward: 112.997090 Avg Reward: 28.970291saving best model....\n",
      "\n",
      "Total T: 95322 Episode Num: 622 Reward: 21.765507 Avg Reward: 29.070847saving best model....\n",
      "\n",
      "Total T: 96132 Episode Num: 629 Reward: 17.177399 Avg Reward: 27.7855491\n",
      "current winratios: [-0.35]\n",
      "\n",
      "Total T: 98204 Episode Num: 644 Reward: 18.623441 Avg Reward: 29.8881184saving best model....\n",
      "\n",
      "Total T: 98594 Episode Num: 646 Reward: 31.376524 Avg Reward: 30.113005saving best model....\n",
      "\n",
      "Total T: 98900 Episode Num: 648 Reward: -1.398457 Avg Reward: 30.298948saving best model....\n",
      "\n",
      "Total T: 99576 Episode Num: 652 Reward: 37.157193 Avg Reward: 31.4454001saving best model....\n",
      "\n",
      "Total T: 99690 Episode Num: 653 Reward: 132.460522 Avg Reward: 32.840808saving best model....\n",
      "\n",
      "Total T: 101612 Episode Num: 667 Reward: 42.488648 Avg Reward: 32.847268saving best model....\n",
      "\n",
      "Total T: 101680 Episode Num: 668 Reward: 111.932540 Avg Reward: 33.979118saving best model....\n",
      "\n",
      "Total T: 109805 Episode Num: 729 Reward: 5.580707 Avg Reward: 21.73053212\n",
      "current winratios: [-0.43]\n",
      "\n",
      "Total T: 122089 Episode Num: 829 Reward: 35.485117 Avg Reward: 24.5262346\n",
      "current winratios: [-0.52]\n",
      "\n",
      "Total T: 134563 Episode Num: 929 Reward: 7.808204 Avg Reward: 34.45954302\n",
      "current winratios: [-0.34]\n",
      "\n",
      "Total T: 138314 Episode Num: 957 Reward: 24.901058 Avg Reward: 33.8930513saving best model....\n",
      "\n",
      "Total T: 138428 Episode Num: 958 Reward: 114.953015 Avg Reward: 34.971701saving best model....\n",
      "\n",
      "Total T: 138530 Episode Num: 959 Reward: 138.508465 Avg Reward: 35.228925saving best model....\n",
      "\n",
      "Total T: 144581 Episode Num: 1006 Reward: 126.845713 Avg Reward: 34.724472saving best model....\n",
      "\n",
      "Total T: 144607 Episode Num: 1007 Reward: 112.663217 Avg Reward: 35.784242saving best model....\n",
      "\n",
      "Total T: 144886 Episode Num: 1009 Reward: 112.762592 Avg Reward: 36.099768saving best model....\n",
      "\n",
      "Total T: 145753 Episode Num: 1014 Reward: 20.572570 Avg Reward: 35.573694saving best model....\n",
      "\n",
      "Total T: 145783 Episode Num: 1015 Reward: 113.614181 Avg Reward: 36.685766saving best model....\n",
      "\n",
      "Total T: 147912 Episode Num: 1029 Reward: 8.020787 Avg Reward: 34.62235261\n",
      "current winratios: [-0.33]\n",
      "\n",
      "Total T: 148748 Episode Num: 1038 Reward: 112.064906 Avg Reward: 37.524148saving best model....\n",
      "\n",
      "Total T: 149187 Episode Num: 1041 Reward: 25.509415 Avg Reward: 37.585490saving best model....\n",
      "\n",
      "Total T: 149398 Episode Num: 1042 Reward: 138.231617 Avg Reward: 38.885590saving best model....\n",
      "\n",
      "Total T: 149649 Episode Num: 1043 Reward: 40.671157 Avg Reward: 39.208387saving best model....\n",
      "\n",
      "Total T: 161156 Episode Num: 1129 Reward: 14.085700 Avg Reward: 32.5191364\n",
      "current winratios: [-0.43]\n",
      "\n",
      "Total T: 172362 Episode Num: 1229 Reward: 13.224213 Avg Reward: 36.1630369\n",
      "current winratios: [-0.44]\n",
      "\n",
      "Total T: 185588 Episode Num: 1329 Reward: 16.894430 Avg Reward: 24.0703113\n",
      "current winratios: [-0.52]\n",
      "\n",
      "Total T: 194471 Episode Num: 1400 Reward: 7.356228 Avg Reward: 38.59020744saving best model....\n",
      "\n",
      "Total T: 194551 Episode Num: 1401 Reward: 112.064271 Avg Reward: 39.620798saving best model....\n",
      "\n",
      "Total T: 194698 Episode Num: 1402 Reward: 18.255843 Avg Reward: 39.819731saving best model....\n",
      "\n",
      "Total T: 194755 Episode Num: 1403 Reward: 11.069958 Avg Reward: 39.867429saving best model....\n",
      "\n",
      "Total T: 195006 Episode Num: 1404 Reward: 27.520815 Avg Reward: 40.157116saving best model....\n",
      "\n",
      "Total T: 195173 Episode Num: 1406 Reward: -1.267448 Avg Reward: 40.177347saving best model....\n",
      "\n",
      "Total T: 195198 Episode Num: 1407 Reward: 112.403053 Avg Reward: 41.125855saving best model....\n",
      "\n",
      "Total T: 195268 Episode Num: 1408 Reward: 4.207434 Avg Reward: 41.174050saving best model....\n",
      "\n",
      "Total T: 195484 Episode Num: 1409 Reward: 54.243277 Avg Reward: 41.660806saving best model....\n",
      "\n",
      "Total T: 195911 Episode Num: 1415 Reward: 114.440728 Avg Reward: 42.446871saving best model....\n",
      "\n",
      "Total T: 196067 Episode Num: 1416 Reward: 131.374316 Avg Reward: 43.682816saving best model....\n",
      "\n",
      "Total T: 197958 Episode Num: 1429 Reward: 13.981600 Avg Reward: 41.7121142\n",
      "current winratios: [-0.27]\n",
      "\n",
      "Total T: 211968 Episode Num: 1529 Reward: 15.274641 Avg Reward: 34.3204606\n",
      "current winratios: [-0.35]\n",
      "\n",
      "Total T: 224052 Episode Num: 1621 Reward: 129.942266 Avg Reward: 44.477274saving best model....\n",
      "\n",
      "Total T: 224303 Episode Num: 1622 Reward: 58.700781 Avg Reward: 44.889610saving best model....\n",
      "\n",
      "Total T: 224554 Episode Num: 1623 Reward: 37.649260 Avg Reward: 45.184645saving best model....\n",
      "\n",
      "Total T: 224879 Episode Num: 1625 Reward: 7.481408 Avg Reward: 45.05978951saving best model....\n",
      "\n",
      "Total T: 224956 Episode Num: 1626 Reward: 121.171221 Avg Reward: 46.101403saving best model....\n",
      "\n",
      "Total T: 225181 Episode Num: 1627 Reward: 24.547608 Avg Reward: 46.168416saving best model....\n",
      "\n",
      "Total T: 225450 Episode Num: 1629 Reward: -1.320414 Avg Reward: 46.284064\n",
      "current winratios: [-0.2]\n",
      "\n",
      "Total T: 231883 Episode Num: 1680 Reward: 121.359971 Avg Reward: 45.827058saving best model....\n",
      "\n",
      "Total T: 231920 Episode Num: 1681 Reward: 111.819665 Avg Reward: 46.851771saving best model....\n",
      "\n",
      "Total T: 232123 Episode Num: 1682 Reward: 38.464618 Avg Reward: 47.249461saving best model....\n",
      "\n",
      "Total T: 232374 Episode Num: 1683 Reward: 31.080062 Avg Reward: 47.386952saving best model....\n",
      "\n",
      "Total T: 232465 Episode Num: 1684 Reward: 15.765666 Avg Reward: 47.458337saving best model....\n",
      "\n",
      "Total T: 232685 Episode Num: 1685 Reward: 135.399848 Avg Reward: 47.598114saving best model....\n",
      "\n",
      "Total T: 232768 Episode Num: 1686 Reward: 8.911379 Avg Reward: 47.700002saving best model....\n",
      "\n",
      "Total T: 232794 Episode Num: 1687 Reward: 112.606076 Avg Reward: 48.456423saving best model....\n",
      "\n",
      "Total T: 232900 Episode Num: 1688 Reward: 16.721563 Avg Reward: 48.518397saving best model....\n",
      "\n",
      "Total T: 232925 Episode Num: 1689 Reward: 111.164069 Avg Reward: 49.347167saving best model....\n",
      "\n",
      "Total T: 233427 Episode Num: 1691 Reward: 14.941709 Avg Reward: 48.485848saving best model....\n",
      "\n",
      "Total T: 233484 Episode Num: 1692 Reward: 111.065374 Avg Reward: 49.608838saving best model....\n",
      "\n",
      "Total T: 233523 Episode Num: 1693 Reward: 111.892019 Avg Reward: 50.558142saving best model....\n",
      "\n",
      "Total T: 233774 Episode Num: 1694 Reward: 37.184059 Avg Reward: 50.848251saving best model....\n",
      "\n",
      "Total T: 233846 Episode Num: 1695 Reward: 121.836262 Avg Reward: 51.936435saving best model....\n",
      "\n",
      "Total T: 233939 Episode Num: 1696 Reward: 119.137007 Avg Reward: 52.783102saving best model....\n",
      "\n",
      "Total T: 237920 Episode Num: 1729 Reward: 19.479147 Avg Reward: 51.0326471\n",
      "current winratios: [-0.12]\n",
      "\n",
      "Total T: 238096 Episode Num: 1731 Reward: 116.477344 Avg Reward: 52.643548saving best model....\n",
      "\n",
      "Total T: 238967 Episode Num: 1737 Reward: 24.600358 Avg Reward: 53.1991329saving best model....\n",
      "\n",
      "Total T: 239149 Episode Num: 1739 Reward: 111.821469 Avg Reward: 54.367886saving best model....\n",
      "\n",
      "Total T: 250653 Episode Num: 1829 Reward: 112.990528 Avg Reward: 42.583688\n",
      "current winratios: [-0.26]\n",
      "\n",
      "Total T: 261973 Episode Num: 1929 Reward: 14.747488 Avg Reward: 43.1242223\n",
      "current winratios: [-0.35]\n",
      "\n",
      "Total T: 274291 Episode Num: 2029 Reward: 7.628981 Avg Reward: 41.02836542\n",
      "current winratios: [-0.35]\n",
      "\n",
      "Total T: 284572 Episode Num: 2129 Reward: 120.778344 Avg Reward: 41.057145\n",
      "current winratios: [-0.35]\n",
      "\n",
      "Total T: 296917 Episode Num: 2229 Reward: 111.079505 Avg Reward: 46.965593\n",
      "current winratios: [-0.21]\n",
      "\n",
      "Total T: 311501 Episode Num: 2329 Reward: 114.268530 Avg Reward: 45.174635\n",
      "current winratios: [-0.27]\n",
      "\n",
      "Total T: 326132 Episode Num: 2429 Reward: 36.091798 Avg Reward: 41.1846064\n",
      "current winratios: [-0.23]\n",
      "\n",
      "Total T: 339858 Episode Num: 2529 Reward: 6.409457 Avg Reward: 41.60180813\n",
      "current winratios: [-0.29]\n",
      "\n",
      "Total T: 352110 Episode Num: 2629 Reward: 130.901417 Avg Reward: 42.578323\n",
      "current winratios: [-0.22]\n",
      "\n",
      "Total T: 362550 Episode Num: 2729 Reward: 126.378183 Avg Reward: 44.498281\n",
      "current winratios: [-0.32]\n",
      "\n",
      "Total T: 375400 Episode Num: 2829 Reward: 25.704215 Avg Reward: 47.5422959\n",
      "current winratios: [-0.21]\n",
      "\n",
      "Total T: 386816 Episode Num: 2929 Reward: 112.669678 Avg Reward: 52.784299\n",
      "current winratios: [-0.15]\n",
      "\n",
      "Total T: 387833 Episode Num: 2943 Reward: 113.206639 Avg Reward: 55.382643saving best model....\n",
      "\n",
      "Total T: 387948 Episode Num: 2944 Reward: 18.295210 Avg Reward: 55.577428saving best model....\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 388431 Episode Num: 2948 Reward: 15.351454 Avg Reward: 55.423457saving best model....\n",
      "\n",
      "Total T: 398315 Episode Num: 3029 Reward: 8.703131 Avg Reward: 49.47938550\n",
      "current winratios: [-0.2]\n",
      "\n",
      "Total T: 412185 Episode Num: 3129 Reward: 32.885554 Avg Reward: 50.0392561\n",
      "current winratios: [-0.17]\n",
      "\n",
      "Total T: 415333 Episode Num: 3149 Reward: 113.666355 Avg Reward: 55.730218saving best model....\n",
      "\n",
      "Total T: 415421 Episode Num: 3150 Reward: 117.688966 Avg Reward: 56.677247saving best model....\n",
      "\n",
      "Total T: 416765 Episode Num: 3161 Reward: 111.820734 Avg Reward: 57.070247saving best model....\n",
      "\n",
      "Total T: 423828 Episode Num: 3212 Reward: 142.043685 Avg Reward: 57.158700saving best model....\n",
      "\n",
      "Total T: 424146 Episode Num: 3215 Reward: 7.466419 Avg Reward: 57.93772861saving best model....\n",
      "\n",
      "Total T: 424208 Episode Num: 3216 Reward: 119.105269 Avg Reward: 58.884032saving best model....\n",
      "\n",
      "Total T: 424459 Episode Num: 3217 Reward: 62.897976 Avg Reward: 59.403949saving best model....\n",
      "\n",
      "Total T: 425275 Episode Num: 3222 Reward: 25.033806 Avg Reward: 58.9602431saving best model....\n",
      "\n",
      "Total T: 425615 Episode Num: 3225 Reward: 112.878075 Avg Reward: 59.666474saving best model....\n",
      "\n",
      "Total T: 426204 Episode Num: 3228 Reward: 119.065524 Avg Reward: 60.654184saving best model....\n",
      "\n",
      "Total T: 426231 Episode Num: 3229 Reward: 112.867373 Avg Reward: 61.454002\n",
      "current winratios: [-0.07]\n",
      "\n",
      "Total T: 426542 Episode Num: 3232 Reward: 15.052231 Avg Reward: 61.3974549saving best model....\n",
      "\n",
      "Total T: 426567 Episode Num: 3233 Reward: 112.997563 Avg Reward: 62.049409saving best model....\n",
      "\n",
      "Total T: 426777 Episode Num: 3234 Reward: 32.963871 Avg Reward: 62.245579saving best model....\n",
      "\n",
      "Total T: 434416 Episode Num: 3285 Reward: 49.049782 Avg Reward: 61.1172273saving best model....\n",
      "\n",
      "Total T: 434633 Episode Num: 3286 Reward: 150.167626 Avg Reward: 62.536014saving best model....\n",
      "\n",
      "Total T: 435032 Episode Num: 3289 Reward: 38.151818 Avg Reward: 63.7332594saving best model....\n",
      "\n",
      "Total T: 435070 Episode Num: 3290 Reward: 110.998051 Avg Reward: 64.320539saving best model....\n",
      "\n",
      "Total T: 440938 Episode Num: 3329 Reward: 112.411445 Avg Reward: 59.633481\n",
      "current winratios: [-0.12]\n",
      "\n",
      "Total T: 454642 Episode Num: 3429 Reward: 144.463607 Avg Reward: 53.406763\n",
      "current winratios: [-0.14]\n",
      "\n",
      "Total T: 465430 Episode Num: 3507 Reward: 25.388956 Avg Reward: 52.0081923"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-86d5ed22ecc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-2bcecce8dae2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agent, env)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             agent.train(replay_buffer, \n\u001b[0m\u001b[1;32m     49\u001b[0m                         \u001b[0mepisode_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                         BATCH_SIZE, GAMMA, TAU, NOISE, NOISE_CLIP, POLICY_FREQUENCY)\n",
      "\u001b[0;32m<ipython-input-6-496645d40395>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, replay_buffer, iterations, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;31m# Delayed policy updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'betas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train agent\n",
    "train(policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.save(filename=\"positive\", directory=\"TD3/saves\")\n",
    "\n",
    "evaluate_policy(policy, env, eval_episodes=100, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.update_weights(add=True)\n",
    "stats = np.array(env.weights).T\n",
    "x,y = stats.shape\n",
    "plt.figure(figsize=(16,5))\n",
    "for i,agent,name in zip(stats,env.agents,['weak','strong']):\n",
    "    plt.plot(np.arange(y-1)*50,i[1:],label=name)\n",
    "plt.legend()\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Winscore')\n",
    "plt.savefig('TD3/Plots/default.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

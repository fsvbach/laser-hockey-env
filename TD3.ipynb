{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twin Delayed Deep Deterministic Policy Gradient (TD3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "from laserhockey.hockey_env import HockeyEnv, BasicOpponent\n",
    "import gym\n",
    "#import roboschool\n",
    "import sys\n",
    "from DDPG.ddpg_agent import DDPGAgent\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Initialize parameters and build model.\n",
    "        Args:\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            max_action (float): highest action to take\n",
    "            seed (int): Random seed\n",
    "            h1_units (int): Number of nodes in first hidden layer\n",
    "            h2_units (int): Number of nodes in second hidden layer\n",
    "            \n",
    "        Return:\n",
    "            action output of network with tanh activation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, action_dim)\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.max_action * torch.tanh(self.l3(x)) \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Initialize parameters and build model.\n",
    "        Args:\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            max_action (float): highest action to take\n",
    "            seed (int): Random seed\n",
    "            h1_units (int): Number of nodes in first hidden layer\n",
    "            h2_units (int): Number of nodes in second hidden layer\n",
    "            \n",
    "        Return:\n",
    "            value output of network \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.l4 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.l5 = nn.Linear(400, 300)\n",
    "        self.l6 = nn.Linear(300, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "\n",
    "        x1 = F.relu(self.l1(xu))\n",
    "        x1 = F.relu(self.l2(x1))\n",
    "        x1 = self.l3(x1)\n",
    "\n",
    "        x2 = F.relu(self.l4(xu))\n",
    "        x2 = F.relu(self.l5(x2))\n",
    "        x2 = self.l6(x2)\n",
    "        return x1, x2\n",
    "\n",
    "\n",
    "    def Q1(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "\n",
    "        x1 = F.relu(self.l1(xu))\n",
    "        x1 = F.relu(self.l2(x1))\n",
    "        x1 = self.l3(x1)\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code based on: \n",
    "# https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
    "\n",
    "# Expects tuples of (state, next_state, action, reward, done)\n",
    "class ReplayBuffer(object):\n",
    "    \"\"\"Buffer to store tuples of experience replay\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size=1000000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_size (int): total amount of tuples to store\n",
    "        \"\"\"\n",
    "        \n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    def add(self, data):\n",
    "        \"\"\"Add experience tuples to buffer\n",
    "        \n",
    "        Args:\n",
    "            data (tuple): experience replay tuple\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = data\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Samples a random amount of experiences from buffer of batch size\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): size of sample\n",
    "        \"\"\"\n",
    "        \n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        states, actions, next_states, rewards, dones = [], [], [], [], []\n",
    "\n",
    "        for i in ind: \n",
    "            s, a, s_, r, d = self.storage[i]\n",
    "            states.append(np.array(s, copy=False))\n",
    "            actions.append(np.array(a, copy=False))\n",
    "            next_states.append(np.array(s_, copy=False))\n",
    "            rewards.append(np.array(r, copy=False))\n",
    "            dones.append(np.array(d, copy=False))\n",
    "\n",
    "        return np.array(states), np.array(actions), np.array(next_states), np.array(rewards).reshape(-1, 1), np.array(dones).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(object):\n",
    "    \"\"\"Agent class that handles the training of the networks and provides outputs as actions\n",
    "    \n",
    "        Args:\n",
    "            state_dim (int): state size\n",
    "            action_dim (int): action size\n",
    "            max_action (float): highest action to take\n",
    "            device (device): cuda or cpu to process tensors\n",
    "            env (env): gym environment to use\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, observation_space, action_space, pretrained=False):\n",
    "        state_dim  = observation_space.shape[0]\n",
    "        action_dim = action_space.shape[0]\n",
    "        max_action = float(action_space.high[0])\n",
    "        \n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-3)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        if pretrained:\n",
    "            self.load(pretrained)\n",
    "\n",
    "\n",
    "        \n",
    "    def act(self, state, eps=0.1):\n",
    "        \"\"\"Select an appropriate action from the agent policy\n",
    "        \n",
    "            Args:\n",
    "                state (array): current state of environment\n",
    "                eps (float): how much noise to add to acitons\n",
    "                \n",
    "            Returns:\n",
    "                action (float): action clipped within action range\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        \n",
    "        action = self.actor(state).cpu().data.numpy().flatten()\n",
    "        if eps != 0: \n",
    "            action = (action + np.random.normal(0, eps, size=self.action_space.shape[0]))\n",
    "            \n",
    "        return action.clip(self.action_space.low, self.action_space.high)\n",
    "\n",
    "    \n",
    "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "        \"\"\"Train and update actor and critic networks\n",
    "        \n",
    "            Args:\n",
    "                replay_buffer (ReplayBuffer): buffer for experience replay\n",
    "                iterations (int): how many times to run training\n",
    "                batch_size(int): batch size to sample from replay buffer\n",
    "                discount (float): discount factor\n",
    "                tau (float): soft update for main networks to target networks\n",
    "                \n",
    "            Return:\n",
    "                actor_loss (float): loss from actor network\n",
    "                critic_loss (float): loss from critic network\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        for it in range(iterations):\n",
    "\n",
    "            # Sample replay buffer \n",
    "            x, y, u, r, d = replay_buffer.sample(batch_size)\n",
    "            state = torch.FloatTensor(x).to(device)\n",
    "            action = torch.FloatTensor(u).to(device)\n",
    "            next_state = torch.FloatTensor(y).to(device)\n",
    "            done = torch.FloatTensor(1 - d).to(device)\n",
    "            reward = torch.FloatTensor(r).to(device)\n",
    "\n",
    "            # Select action according to policy and add clipped noise \n",
    "            noise = torch.FloatTensor(u).data.normal_(0, policy_noise).to(device)\n",
    "            noise = noise.clamp(-noise_clip, noise_clip)\n",
    "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            # Compute the target Q value\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + (done * discount * target_Q).detach()\n",
    "\n",
    "            # Get current Q estimates\n",
    "            current_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "            # Compute critic loss\n",
    "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q) \n",
    "\n",
    "            # Optimize the critic\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            # Delayed policy updates\n",
    "            if it % policy_freq == 0:\n",
    "\n",
    "                # Compute actor loss\n",
    "                actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "\n",
    "                # Optimize the actor \n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                # Update the frozen target models\n",
    "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "\n",
    "    def save(self, filename, directory):\n",
    "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "\n",
    "\n",
    "    def load(self, filename=\"best_avg\", directory=\"TD3/saves\"):\n",
    "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner():\n",
    "    \"\"\"Carries out the environment steps and adds experiences to memory\"\"\"\n",
    "    \n",
    "    def __init__(self, env, agent, replay_buffer):\n",
    "        \n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.obs = env.reset()\n",
    "        self.done = False\n",
    "        \n",
    "    def next_step(self, episode_timesteps, noise=0.1):\n",
    "        \n",
    "        action = self.agent.act(np.array(self.obs), eps=0.1)\n",
    "\n",
    "        # Perform action\n",
    "        new_obs, reward, done, info = self.env.step(action) \n",
    "        done_bool = 0 if episode_timesteps + 1 == 250 else float(done)\n",
    "        \n",
    "        #add proxy\n",
    "        reward += np.inner(list(info.values()), np.array([40,200,5,100,1]))\n",
    "        \n",
    "        # Store data in replay buffer\n",
    "        replay_buffer.add((self.obs, new_obs, action, reward, done_bool))\n",
    "        \n",
    "        self.obs = new_obs\n",
    "        \n",
    "        if done:\n",
    "            self.obs = self.env.reset()\n",
    "            done = False\n",
    "            \n",
    "            return reward, True\n",
    "        \n",
    "        return reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, env, eval_episodes=100,render=False):\n",
    "    \"\"\"run several episodes using the best agent policy\n",
    "        \n",
    "        Args:\n",
    "            policy (agent): agent to evaluate\n",
    "            env (env): gym environment\n",
    "            eval_episodes (int): how many test episodes to run\n",
    "            render (bool): show training\n",
    "        \n",
    "        Returns:\n",
    "            avg_reward (float): average reward over the number of evaluations\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    avg_reward = 0.\n",
    "    for i in range(eval_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = policy.act(np.array(obs), eps=0)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "\n",
    "    print(\"\\n---------------------------------------\")\n",
    "    print(\"Evaluation over {:d} episodes: {:f}\" .format(eval_episodes, avg_reward))\n",
    "    print(\"---------------------------------------\")\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe(env,replay_buffer, observation_steps):\n",
    "    \"\"\"run episodes while taking random actions and filling replay_buffer\n",
    "    \n",
    "        Args:\n",
    "            env (env): gym environment\n",
    "            replay_buffer(ReplayBuffer): buffer to store experience replay\n",
    "            observation_steps (int): how many steps to observe for\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    time_steps = 0\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while time_steps < observation_steps:\n",
    "        action = env.action_space.sample()\n",
    "        new_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        replay_buffer.add((obs, new_obs, action, reward, done))\n",
    "\n",
    "        obs = new_obs\n",
    "        time_steps += 1\n",
    "\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "\n",
    "        print(\"\\rPopulating Buffer {}/{}.\".format(time_steps, observation_steps), end=\"\")\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, test_env):\n",
    "    \"\"\"Train the agent for exploration steps\n",
    "    \n",
    "        Args:\n",
    "            agent (Agent): agent to use\n",
    "            env (environment): gym environment\n",
    "            writer (SummaryWriter): tensorboard writer\n",
    "            exploration (int): how many training steps to run\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    total_timesteps = 0\n",
    "    timesteps_since_eval = 0\n",
    "    episode_num = 0\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    done = False \n",
    "    obs = env.reset()\n",
    "    evaluations = []\n",
    "    rewards = []\n",
    "    best_avg = -2000\n",
    "    \n",
    "    writer = SummaryWriter(comment=\"-new_rewards\")\n",
    "    \n",
    "    while total_timesteps < EXPLORATION:\n",
    "    \n",
    "        if done: \n",
    "\n",
    "            if total_timesteps != 0: \n",
    "                rewards.append(episode_reward)\n",
    "                avg_reward = np.mean(rewards[-100:])\n",
    "                \n",
    "                writer.add_scalar(\"avg_reward\", avg_reward, total_timesteps)\n",
    "                writer.add_scalar(\"reward_step\", reward, total_timesteps)\n",
    "                writer.add_scalar(\"episode_reward\", episode_reward, total_timesteps)\n",
    "                \n",
    "                if best_avg < avg_reward:\n",
    "                    best_avg = avg_reward\n",
    "                    print(\"saving best model....\\n\")\n",
    "                    agent.save(\"best_avg\",\"TD3/saves\")\n",
    "\n",
    "                print(\"\\rTotal T: {:d} Episode Num: {:d} Reward: {:f} Avg Reward: {:f}\".format(\n",
    "                    total_timesteps, episode_num, episode_reward, avg_reward), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "\n",
    "                if avg_reward >= REWARD_THRESH:\n",
    "                    break\n",
    "\n",
    "                agent.train(replay_buffer, episode_timesteps, BATCH_SIZE, GAMMA, TAU, NOISE, NOISE_CLIP, POLICY_FREQUENCY)\n",
    "\n",
    "                # Evaluate episode\n",
    "#                 if timesteps_since_eval >= EVAL_FREQUENCY:\n",
    "#                     timesteps_since_eval %= EVAL_FREQUENCY\n",
    "#                     eval_reward = evaluate_policy(agent, test_env)\n",
    "#                     evaluations.append(avg_reward)\n",
    "#                     writer.add_scalar(\"eval_reward\", eval_reward, total_timesteps)\n",
    "\n",
    "#                     if best_avg < eval_reward:\n",
    "#                         best_avg = eval_reward\n",
    "#                         print(\"saving best model....\\n\")\n",
    "#                         agent.save(\"best_avg\",\"saves\")\n",
    "\n",
    "                episode_reward = 0\n",
    "                episode_timesteps = 0\n",
    "                episode_num += 1 \n",
    "\n",
    "        reward, done = runner.next_step(episode_timesteps)\n",
    "        episode_reward += reward\n",
    "\n",
    "        episode_timesteps += 1\n",
    "        total_timesteps += 1\n",
    "        timesteps_since_eval += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingHall(HockeyEnv):\n",
    "\n",
    "  def __init__(self, opponent):\n",
    "    super().__init__(mode=HockeyEnv.NORMAL, keep_mode=True)\n",
    "    self.opponent = opponent\n",
    "    # linear force in (x,y)-direction, torque, and shooting\n",
    "    self.action_space = spaces.Box(-1, +1, (4,), dtype=np.float32)\n",
    "\n",
    "  def step(self, action):\n",
    "    ob2 = self.obs_agent_two()\n",
    "    a2 = self.opponent.act(ob2)\n",
    "    action2 = np.hstack([action, a2])\n",
    "    return super().step(action2)\n",
    "\n",
    "#env = TrainingHall()\n",
    "weak = BasicOpponent(weak=False)\n",
    "#td3 = TD3(env.observation_space, env.action_space, pretrained='weak')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TrainingHall(weak)\n",
    "SEED = 0\n",
    "OBSERVATION = 10000\n",
    "EXPLORATION = 500000\n",
    "BATCH_SIZE = 100\n",
    "GAMMA = 0.99\n",
    "TAU = 0.005\n",
    "NOISE = 0.2\n",
    "NOISE_CLIP = 0.5\n",
    "EXPLORE_NOISE = 0.1\n",
    "POLICY_FREQUENCY = 2\n",
    "EVAL_FREQUENCY = 5000\n",
    "REWARD_THRESH = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set seeds\n",
    "env.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "policy = TD3(env.observation_space, env.action_space, pretrained='TDweak')\n",
    "\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "runner = Runner(env, policy, replay_buffer)\n",
    "\n",
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating Buffer 10000/10000."
     ]
    }
   ],
   "source": [
    "# Populate replay buffer\n",
    "observe(env, replay_buffer, OBSERVATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving best model....\n",
      "\n",
      "Total T: 49 Episode Num: 0 Reward: -69.801231 Avg Reward: -69.801231saving best model....\n",
      "\n",
      "Total T: 21534 Episode Num: 134 Reward: 48.260448 Avg Reward: -49.56497919saving best model....\n",
      "\n",
      "Total T: 94274 Episode Num: 520 Reward: 33.599673 Avg Reward: -49.13449107saving best model....\n",
      "\n",
      "Total T: 94638 Episode Num: 522 Reward: -67.925388 Avg Reward: -47.684522saving best model....\n",
      "\n",
      "Total T: 94919 Episode Num: 524 Reward: -57.993004 Avg Reward: -47.455164saving best model....\n",
      "\n",
      "Total T: 95170 Episode Num: 525 Reward: -37.949911 Avg Reward: -46.068287saving best model....\n",
      "\n",
      "Total T: 95373 Episode Num: 526 Reward: -53.662495 Avg Reward: -45.957148saving best model....\n",
      "\n",
      "Total T: 95422 Episode Num: 527 Reward: 48.816554 Avg Reward: -44.207833saving best model....\n",
      "\n",
      "Total T: 95525 Episode Num: 528 Reward: -61.811235 Avg Reward: -44.124011saving best model....\n",
      "\n",
      "Total T: 95776 Episode Num: 529 Reward: -73.991460 Avg Reward: -43.754274saving best model....\n",
      "\n",
      "Total T: 95963 Episode Num: 530 Reward: -42.424128 Avg Reward: -43.446398saving best model....\n",
      "\n",
      "Total T: 97346 Episode Num: 538 Reward: -52.582140 Avg Reward: -43.161963saving best model....\n",
      "\n",
      "Total T: 97660 Episode Num: 540 Reward: -84.349103 Avg Reward: -42.575242saving best model....\n",
      "\n",
      "Total T: 97911 Episode Num: 541 Reward: -0.453686 Avg Reward: -41.187061saving best model....\n",
      "\n",
      "Total T: 98162 Episode Num: 542 Reward: -1.907575 Avg Reward: -40.404230saving best model....\n",
      "\n",
      "Total T: 98340 Episode Num: 543 Reward: 12.618224 Avg Reward: -39.487742saving best model....\n",
      "\n",
      "Total T: 98799 Episode Num: 547 Reward: 49.020280 Avg Reward: -39.0529348saving best model....\n",
      "\n",
      "Total T: 98971 Episode Num: 548 Reward: 42.710855 Avg Reward: -38.008037saving best model....\n",
      "\n",
      "Total T: 99170 Episode Num: 550 Reward: -60.280589 Avg Reward: -37.218468saving best model....\n",
      "\n",
      "Total T: 99241 Episode Num: 551 Reward: 44.794316 Avg Reward: -36.114927saving best model....\n",
      "\n",
      "Total T: 99364 Episode Num: 552 Reward: -50.254664 Avg Reward: -36.004442saving best model....\n",
      "\n",
      "Total T: 100118 Episode Num: 557 Reward: -118.933435 Avg Reward: -36.117612saving best model....\n",
      "\n",
      "Total T: 103849 Episode Num: 579 Reward: -109.562707 Avg Reward: -35.881216saving best model....\n",
      "\n",
      "Total T: 104100 Episode Num: 580 Reward: -5.221111 Avg Reward: -35.064993saving best model....\n",
      "\n",
      "Total T: 104215 Episode Num: 581 Reward: 55.395749 Avg Reward: -33.821823saving best model....\n",
      "\n",
      "Total T: 104456 Episode Num: 582 Reward: 39.416996 Avg Reward: -32.823081saving best model....\n",
      "\n",
      "Total T: 130871 Episode Num: 716 Reward: -3.435910 Avg Reward: -33.57551606saving best model....\n",
      "\n",
      "Total T: 131832 Episode Num: 722 Reward: -29.274318 Avg Reward: -32.958874saving best model....\n",
      "\n",
      "Total T: 132083 Episode Num: 723 Reward: 2.321543 Avg Reward: -32.052181saving best model....\n",
      "\n",
      "Total T: 132334 Episode Num: 724 Reward: 8.179573 Avg Reward: -31.353454saving best model....\n",
      "\n",
      "Total T: 132585 Episode Num: 725 Reward: -10.447884 Avg Reward: -30.709396saving best model....\n",
      "\n",
      "Total T: 132836 Episode Num: 726 Reward: 4.143565 Avg Reward: -29.971081saving best model....\n",
      "\n",
      "Total T: 133087 Episode Num: 727 Reward: -34.983609 Avg Reward: -29.549985saving best model....\n",
      "\n",
      "Total T: 133469 Episode Num: 729 Reward: -74.915917 Avg Reward: -29.465691saving best model....\n",
      "\n",
      "Total T: 133529 Episode Num: 730 Reward: 51.292905 Avg Reward: -28.314795saving best model....\n",
      "\n",
      "Total T: 133851 Episode Num: 732 Reward: -60.916612 Avg Reward: -28.182588saving best model....\n",
      "\n",
      "Total T: 134196 Episode Num: 734 Reward: -61.099404 Avg Reward: -27.835544saving best model....\n",
      "\n",
      "Total T: 135291 Episode Num: 741 Reward: 1.167397 Avg Reward: -27.94125912saving best model....\n",
      "\n",
      "Total T: 135542 Episode Num: 742 Reward: -9.957585 Avg Reward: -27.394248saving best model....\n",
      "\n",
      "Total T: 135793 Episode Num: 743 Reward: 5.278804 Avg Reward: -27.034025saving best model....\n",
      "\n",
      "Total T: 137600 Episode Num: 752 Reward: -7.560886 Avg Reward: -26.9304015saving best model....\n",
      "\n",
      "Total T: 138165 Episode Num: 755 Reward: 11.984264 Avg Reward: -25.6415827saving best model....\n",
      "\n",
      "Total T: 140481 Episode Num: 766 Reward: -4.476789 Avg Reward: -25.8379505saving best model....\n",
      "\n",
      "Total T: 140983 Episode Num: 768 Reward: -2.607254 Avg Reward: -24.748809saving best model....\n",
      "\n",
      "Total T: 144042 Episode Num: 784 Reward: -60.680428 Avg Reward: -25.055015saving best model....\n",
      "\n",
      "Total T: 144293 Episode Num: 785 Reward: 10.240016 Avg Reward: -24.187278saving best model....\n",
      "\n",
      "Total T: 148480 Episode Num: 806 Reward: 55.944233 Avg Reward: -24.3079404saving best model....\n",
      "\n",
      "Total T: 148731 Episode Num: 807 Reward: 9.506804 Avg Reward: -23.352766saving best model....\n",
      "\n",
      "Total T: 151850 Episode Num: 821 Reward: 7.486210 Avg Reward: -23.55606003saving best model....\n",
      "\n",
      "Total T: 154883 Episode Num: 836 Reward: -5.034615 Avg Reward: -23.2949959saving best model....\n",
      "\n",
      "Total T: 154912 Episode Num: 837 Reward: 55.524616 Avg Reward: -21.994954saving best model....\n",
      "\n",
      "Total T: 155163 Episode Num: 838 Reward: -14.193131 Avg Reward: -21.633704saving best model....\n",
      "\n",
      "Total T: 155414 Episode Num: 839 Reward: -3.955413 Avg Reward: -20.982224saving best model....\n",
      "\n",
      "Total T: 157362 Episode Num: 849 Reward: 4.992120 Avg Reward: -20.08722058saving best model....\n",
      "\n",
      "Total T: 157490 Episode Num: 850 Reward: 55.007195 Avg Reward: -19.582478saving best model....\n",
      "\n",
      "Total T: 158962 Episode Num: 857 Reward: 4.117213 Avg Reward: -20.00013566saving best model....\n",
      "\n",
      "Total T: 159213 Episode Num: 858 Reward: -18.914888 Avg Reward: -19.362904saving best model....\n",
      "\n",
      "Total T: 159464 Episode Num: 859 Reward: 4.394328 Avg Reward: -19.256314saving best model....\n",
      "\n",
      "Total T: 159538 Episode Num: 860 Reward: -58.512112 Avg Reward: -19.226892saving best model....\n",
      "\n",
      "Total T: 159704 Episode Num: 862 Reward: -60.609522 Avg Reward: -18.937818saving best model....\n",
      "\n",
      "Total T: 161338 Episode Num: 870 Reward: -3.144137 Avg Reward: -18.9106745saving best model....\n",
      "\n",
      "Total T: 161589 Episode Num: 871 Reward: 2.341363 Avg Reward: -18.053907saving best model....\n",
      "\n",
      "Total T: 161840 Episode Num: 872 Reward: 4.741537 Avg Reward: -17.451667saving best model....\n",
      "\n",
      "Total T: 162342 Episode Num: 874 Reward: -5.488444 Avg Reward: -16.869415saving best model....\n",
      "\n",
      "Total T: 164891 Episode Num: 888 Reward: -4.475175 Avg Reward: -17.0756626saving best model....\n",
      "\n",
      "Total T: 165644 Episode Num: 893 Reward: 7.166166 Avg Reward: -16.65904210saving best model....\n",
      "\n",
      "Total T: 165718 Episode Num: 894 Reward: 53.431798 Avg Reward: -15.419558saving best model....\n",
      "\n",
      "Total T: 165969 Episode Num: 895 Reward: 7.436341 Avg Reward: -14.743009saving best model....\n",
      "\n",
      "Total T: 166933 Episode Num: 900 Reward: 5.215290 Avg Reward: -14.69453606saving best model....\n",
      "\n",
      "Total T: 167184 Episode Num: 901 Reward: 1.696021 Avg Reward: -13.743509saving best model....\n",
      "\n",
      "Total T: 167353 Episode Num: 902 Reward: 43.590909 Avg Reward: -13.271063saving best model....\n",
      "\n",
      "Total T: 167604 Episode Num: 903 Reward: 10.298925 Avg Reward: -13.204807saving best model....\n",
      "\n",
      "Total T: 177386 Episode Num: 959 Reward: 7.633007 Avg Reward: -13.13125542saving best model....\n",
      "\n",
      "Total T: 191978 Episode Num: 1039 Reward: 2.219150 Avg Reward: -12.41802212saving best model....\n",
      "\n",
      "Total T: 205671 Episode Num: 1114 Reward: 27.862977 Avg Reward: -13.0296932saving best model....\n",
      "\n",
      "Total T: 206233 Episode Num: 1117 Reward: 9.838708 Avg Reward: -12.10161544saving best model....\n",
      "\n",
      "Total T: 213683 Episode Num: 1160 Reward: 51.563488 Avg Reward: -11.6592866saving best model....\n",
      "\n",
      "Total T: 213934 Episode Num: 1161 Reward: -19.721550 Avg Reward: -11.140091saving best model....\n",
      "\n",
      "Total T: 214185 Episode Num: 1162 Reward: -5.622246 Avg Reward: -10.725249saving best model....\n",
      "\n",
      "Total T: 214687 Episode Num: 1164 Reward: 8.399083 Avg Reward: -10.591785saving best model....\n",
      "\n",
      "Total T: 214938 Episode Num: 1165 Reward: 22.376067 Avg Reward: -9.813484saving best model....\n",
      "\n",
      "Total T: 215277 Episode Num: 1167 Reward: 6.408738 Avg Reward: -9.3274301saving best model....\n",
      "\n",
      "Total T: 217948 Episode Num: 1180 Reward: 48.506056 Avg Reward: -8.98280206saving best model....\n",
      "\n",
      "Total T: 218199 Episode Num: 1181 Reward: 7.770117 Avg Reward: -8.329297saving best model....\n",
      "\n",
      "Total T: 218450 Episode Num: 1182 Reward: 3.476455 Avg Reward: -7.700097saving best model....\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 218606 Episode Num: 1183 Reward: -57.405443 Avg Reward: -7.679716saving best model....\n",
      "\n",
      "Total T: 218666 Episode Num: 1184 Reward: 44.587084 Avg Reward: -7.244158saving best model....\n",
      "\n",
      "Total T: 218917 Episode Num: 1185 Reward: 2.700310 Avg Reward: -7.232512saving best model....\n",
      "\n",
      "Total T: 219419 Episode Num: 1187 Reward: 1.696952 Avg Reward: -7.5731739saving best model....\n",
      "\n",
      "Total T: 219776 Episode Num: 1189 Reward: 1.354024 Avg Reward: -7.0122260saving best model....\n",
      "\n",
      "Total T: 220278 Episode Num: 1191 Reward: -3.874139 Avg Reward: -6.853545saving best model....\n",
      "\n",
      "Total T: 220529 Episode Num: 1192 Reward: -0.263897 Avg Reward: -6.247535saving best model....\n",
      "\n",
      "Total T: 220780 Episode Num: 1193 Reward: -7.874359 Avg Reward: -5.795920saving best model....\n",
      "\n",
      "Total T: 222457 Episode Num: 1203 Reward: 6.176538 Avg Reward: -5.67795566saving best model....\n",
      "\n",
      "Total T: 227837 Episode Num: 1229 Reward: 2.845030 Avg Reward: -6.16995174saving best model....\n",
      "\n",
      "Total T: 228846 Episode Num: 1235 Reward: 4.701892 Avg Reward: -5.04496873saving best model....\n",
      "\n",
      "Total T: 229097 Episode Num: 1236 Reward: 7.911237 Avg Reward: -4.988795saving best model....\n",
      "\n",
      "Total T: 260251 Episode Num: 1391 Reward: 3.982888 Avg Reward: -5.124152803saving best model....\n",
      "\n",
      "Total T: 260753 Episode Num: 1393 Reward: 4.252223 Avg Reward: -4.426637saving best model....\n",
      "\n",
      "Total T: 306261 Episode Num: 1607 Reward: 7.502116 Avg Reward: -4.18752143saving best model....\n",
      "\n",
      "Total T: 306395 Episode Num: 1608 Reward: 61.323304 Avg Reward: -3.602042saving best model....\n",
      "\n",
      "Total T: 306623 Episode Num: 1609 Reward: 68.071113 Avg Reward: -2.992695saving best model....\n",
      "\n",
      "Total T: 306966 Episode Num: 1611 Reward: 5.625781 Avg Reward: -2.6071018saving best model....\n",
      "\n",
      "Total T: 309132 Episode Num: 1621 Reward: 37.507244 Avg Reward: -2.9405070saving best model....\n",
      "\n",
      "Total T: 309338 Episode Num: 1622 Reward: 57.782703 Avg Reward: -1.776296saving best model....\n",
      "\n",
      "Total T: 309675 Episode Num: 1624 Reward: 49.735370 Avg Reward: -1.281527saving best model....\n",
      "\n",
      "Total T: 309850 Episode Num: 1625 Reward: 54.018410 Avg Reward: -0.187734saving best model....\n",
      "\n",
      "Total T: 311809 Episode Num: 1635 Reward: 3.047873 Avg Reward: 0.520736923saving best model....\n",
      "\n",
      "Total T: 311873 Episode Num: 1636 Reward: 57.479013 Avg Reward: 1.209293saving best model....\n",
      "\n",
      "Total T: 313264 Episode Num: 1643 Reward: 3.039093 Avg Reward: 1.55110426saving best model....\n",
      "\n",
      "Total T: 313614 Episode Num: 1645 Reward: 12.227319 Avg Reward: 1.455583saving best model....\n",
      "\n",
      "Total T: 314479 Episode Num: 1650 Reward: 43.288414 Avg Reward: 2.4741114saving best model....\n",
      "\n",
      "Total T: 314643 Episode Num: 1651 Reward: 37.280731 Avg Reward: 3.365328saving best model....\n",
      "\n",
      "Total T: 314894 Episode Num: 1652 Reward: 5.736008 Avg Reward: 3.504191saving best model....\n",
      "\n",
      "Total T: 315145 Episode Num: 1653 Reward: 5.949583 Avg Reward: 3.508111saving best model....\n",
      "\n",
      "Total T: 316080 Episode Num: 1659 Reward: -2.031986 Avg Reward: 3.7897276saving best model....\n",
      "\n",
      "Total T: 316195 Episode Num: 1660 Reward: 61.489283 Avg Reward: 4.564825saving best model....\n",
      "\n",
      "Total T: 316446 Episode Num: 1661 Reward: 6.765845 Avg Reward: 5.169264saving best model....\n",
      "\n",
      "Total T: 316525 Episode Num: 1662 Reward: 56.968372 Avg Reward: 5.793205saving best model....\n",
      "\n",
      "Total T: 316776 Episode Num: 1663 Reward: 5.957479 Avg Reward: 5.899565saving best model....\n",
      "\n",
      "Total T: 469215 Episode Num: 2432 Reward: -0.535744 Avg Reward: -1.0125907"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-86d5ed22ecc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-82-54c91622b878>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agent, test_env)\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTAU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNOISE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNOISE_CLIP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPOLICY_FREQUENCY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;31m# Evaluate episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-ac3d95726fa0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, replay_buffer, iterations, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;31m# Optimize the critic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train agent\n",
    "train(policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------\n",
      "Evaluation over 100 episodes: 0.300000\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.save(filename=\"best_avg\", directory=\"TD3/saves\")\n",
    "\n",
    "evaluate_policy(policy, env, eval_episodes=100, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

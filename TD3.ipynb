{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twin Delayed Deep Deterministic Policy Gradient (TD3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "from laserhockey.hockey_env import BasicOpponent, HockeyEnv_BasicOpponent\n",
    "from laserhockey.TrainingHall import TrainingHall\n",
    "import gym\n",
    "#import roboschool\n",
    "import sys\n",
    "from DDPG.ddpg_agent import DDPGAgent\n",
    "from DQN.agent import DQNAgent\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Initialize parameters and build model.\n",
    "        Args:\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            max_action (float): highest action to take\n",
    "            seed (int): Random seed\n",
    "            h1_units (int): Number of nodes in first hidden layer\n",
    "            h2_units (int): Number of nodes in second hidden layer\n",
    "            \n",
    "        Return:\n",
    "            action output of network with tanh activation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, action_dim)\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.max_action * torch.tanh(self.l3(x)) \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Initialize parameters and build model.\n",
    "        Args:\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            max_action (float): highest action to take\n",
    "            seed (int): Random seed\n",
    "            h1_units (int): Number of nodes in first hidden layer\n",
    "            h2_units (int): Number of nodes in second hidden layer\n",
    "            \n",
    "        Return:\n",
    "            value output of network \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.l4 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.l5 = nn.Linear(400, 300)\n",
    "        self.l6 = nn.Linear(300, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "\n",
    "        x1 = F.relu(self.l1(xu))\n",
    "        x1 = F.relu(self.l2(x1))\n",
    "        x1 = self.l3(x1)\n",
    "\n",
    "        x2 = F.relu(self.l4(xu))\n",
    "        x2 = F.relu(self.l5(x2))\n",
    "        x2 = self.l6(x2)\n",
    "        return x1, x2\n",
    "\n",
    "\n",
    "    def Q1(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "\n",
    "        x1 = F.relu(self.l1(xu))\n",
    "        x1 = F.relu(self.l2(x1))\n",
    "        x1 = self.l3(x1)\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code based on: \n",
    "# https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
    "\n",
    "# Expects tuples of (state, next_state, action, reward, done)\n",
    "class ReplayBuffer(object):\n",
    "    \"\"\"Buffer to store tuples of experience replay\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size=150000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_size (int): total amount of tuples to store\n",
    "        \"\"\"\n",
    "        \n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    def add(self, data):\n",
    "        \"\"\"Add experience tuples to buffer\n",
    "        \n",
    "        Args:\n",
    "            data (tuple): experience replay tuple\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = data\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Samples a random amount of experiences from buffer of batch size\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): size of sample\n",
    "        \"\"\"\n",
    "        \n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        states, actions, next_states, rewards, dones = [], [], [], [], []\n",
    "\n",
    "        for i in ind: \n",
    "            s, a, s_, r, d = self.storage[i]\n",
    "            states.append(np.array(s, copy=False))\n",
    "            actions.append(np.array(a, copy=False))\n",
    "            next_states.append(np.array(s_, copy=False))\n",
    "            rewards.append(np.array(r, copy=False))\n",
    "            dones.append(np.array(d, copy=False))\n",
    "\n",
    "        return np.array(states), np.array(actions), np.array(next_states), np.array(rewards).reshape(-1, 1), np.array(dones).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(object):\n",
    "    \"\"\"Agent class that handles the training of the networks and provides outputs as actions\n",
    "    \n",
    "        Args:\n",
    "            state_dim (int): state size\n",
    "            action_dim (int): action size\n",
    "            max_action (float): highest action to take\n",
    "            device (device): cuda or cpu to process tensors\n",
    "            env (env): gym environment to use\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, observation_space, action_space, pretrained=False):\n",
    "        state_dim  = observation_space.shape[0]\n",
    "        action_dim = action_space.shape[0]\n",
    "        max_action = float(action_space.high[0])\n",
    "        \n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-3)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        #self.scaling = torch.tensor([ 5.0,  5.0 , 5.0, 3.0, 3.0, 3.0,  \n",
    "        #                                2.0,  2.0,  1.0, 1.0, 1.0, 1.0,  \n",
    "        #                               10.0, 10.0, 10.0, 10.0, 5.0 ,5.0])\n",
    "        if pretrained:\n",
    "            self.load(pretrained)\n",
    "\n",
    "\n",
    "        \n",
    "    def act(self, state, eps=0.0):\n",
    "        \"\"\"Select an appropriate action from the agent policy\n",
    "        \n",
    "            Args:\n",
    "                state (array): current state of environment\n",
    "                eps (float): how much noise to add to acitons\n",
    "                \n",
    "            Returns:\n",
    "                action (float): action clipped within action range\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)#*self.scaling\n",
    "        action = self.actor(state).cpu().data.numpy().flatten()\n",
    "        if eps != 0: \n",
    "            action = (action + np.random.normal(0, eps, size=self.action_space.shape[0]))\n",
    "            \n",
    "        return action.clip(self.action_space.low, self.action_space.high)\n",
    "\n",
    "    \n",
    "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "        \"\"\"Train and update actor and critic networks\n",
    "        \n",
    "            Args:\n",
    "                replay_buffer (ReplayBuffer): buffer for experience replay\n",
    "                iterations (int): how many times to run training\n",
    "                batch_size(int): batch size to sample from replay buffer\n",
    "                discount (float): discount factor\n",
    "                tau (float): soft update for main networks to target networks\n",
    "                \n",
    "            Return:\n",
    "                actor_loss (float): loss from actor network\n",
    "                critic_loss (float): loss from critic network\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        for it in range(iterations):\n",
    "\n",
    "            # Sample replay buffer \n",
    "            x, y, u, r, d = replay_buffer.sample(batch_size)\n",
    "            state = torch.FloatTensor(x).to(device)#*self.scaling\n",
    "            action = torch.FloatTensor(u).to(device)\n",
    "            next_state = torch.FloatTensor(y).to(device)#*self.scaling\n",
    "            done = torch.FloatTensor(1 - d).to(device)\n",
    "            reward = torch.FloatTensor(r).to(device)\n",
    "            \n",
    "            # #add noise to observation\n",
    "            #clean_idx = (torch.tensor([ 0,1,2,3,4,5,12,13,14,15, 16, 17]),)\n",
    "            #noise = torch.FloatTensor(state).data.normal_(0, 1).to(device)\n",
    "            #noise[clean_idx] = 0\n",
    "            #state += noise\n",
    "\n",
    "            # Select action according to policy and add clipped noise \n",
    "            noise = torch.FloatTensor(u).data.normal_(0, policy_noise).to(device)\n",
    "            noise = noise.clamp(-noise_clip, noise_clip)\n",
    "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            # Compute the target Q value\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + (done * discount * target_Q).detach()\n",
    "\n",
    "            # Get current Q estimates\n",
    "            current_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "            # Compute critic loss\n",
    "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q) \n",
    "\n",
    "            # Optimize the critic\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            # Delayed policy updates\n",
    "            if it % policy_freq == 0:\n",
    "\n",
    "                # Compute actor loss\n",
    "                actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "\n",
    "                # Optimize the actor \n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                # Update the frozen target models\n",
    "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "\n",
    "    def save(self, filename, directory):\n",
    "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "\n",
    "\n",
    "    def load(self, filename=\"best_avg\", directory=\"TD3/saves\"):\n",
    "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner():\n",
    "    \"\"\"Carries out the environment steps and adds experiences to memory\"\"\"\n",
    "    \n",
    "    def __init__(self, env, agent, replay_buffer):\n",
    "        \n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.obs = env.reset()\n",
    "        self.done = False\n",
    "        \n",
    "    def next_step(self, episode_timesteps, noise=0.1):\n",
    "        \n",
    "        action = self.agent.act(np.array(self.obs), eps=0.1)\n",
    "\n",
    "        # Perform action\n",
    "        new_obs, reward, done, info = self.env.step(action) \n",
    "        done_bool = 0 if episode_timesteps + 1 == 255 else float(done)\n",
    "        \n",
    "        #add proxy\n",
    "        reward += sum(list(info.values()))\n",
    "        \n",
    "        # Store data in replay buffer\n",
    "        replay_buffer.add((self.obs, new_obs, action, reward, done_bool))\n",
    "        \n",
    "        self.obs = new_obs\n",
    "        \n",
    "        if done:\n",
    "            self.obs = self.env.reset()\n",
    "            done = False\n",
    "            \n",
    "            return reward, True\n",
    "        \n",
    "        return reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, env, eval_episodes=100,render=False):\n",
    "    \"\"\"run several episodes using the best agent policy\n",
    "        \n",
    "        Args:\n",
    "            policy (agent): agent to evaluate\n",
    "            env (env): gym environment\n",
    "            eval_episodes (int): how many test episodes to run\n",
    "            render (bool): show training\n",
    "        \n",
    "        Returns:\n",
    "            avg_reward (float): average reward over the number of evaluations\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    avg_reward = 0.\n",
    "    for i in range(eval_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = policy.act(np.array(obs), eps=0)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "\n",
    "    print(\"\\n---------------------------------------\")\n",
    "    print(\"Evaluation over {:d} episodes: {:f}\" .format(eval_episodes, avg_reward))\n",
    "    print(\"---------------------------------------\")\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe(env,replay_buffer, observation_steps):\n",
    "    \"\"\"run episodes while taking random actions and filling replay_buffer\n",
    "    \n",
    "        Args:\n",
    "            env (env): gym environment\n",
    "            replay_buffer(ReplayBuffer): buffer to store experience replay\n",
    "            observation_steps (int): how many steps to observe for\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    time_steps = 0\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while time_steps < observation_steps:\n",
    "        action = env.action_space.sample()\n",
    "        new_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        replay_buffer.add((obs, new_obs, action, reward, done))\n",
    "\n",
    "        obs = new_obs\n",
    "        time_steps += 1\n",
    "\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "\n",
    "        print(\"\\rPopulating Buffer {}/{}.\".format(time_steps, observation_steps), end=\"\")\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env):\n",
    "    \"\"\"Train the agent for exploration steps\n",
    "    \n",
    "        Args:\n",
    "            agent (Agent): agent to use\n",
    "            env (environment): gym environment\n",
    "            writer (SummaryWriter): tensorboard writer\n",
    "            exploration (int): how many training steps to run\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    total_timesteps = 0\n",
    "    timesteps_since_eval = 0\n",
    "    episode_num = 0\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    done = False \n",
    "    evaluations = []\n",
    "    rewards = []\n",
    "    best_avg = -5\n",
    "    \n",
    "    writer = SummaryWriter(comment=\"-last\")\n",
    "    \n",
    "    while total_timesteps < EXPLORATION:\n",
    "    \n",
    "        if done and total_timesteps != 0:\n",
    "\n",
    "            rewards.append(episode_reward)\n",
    "            avg_reward = np.mean(rewards[-100:])\n",
    "\n",
    "            writer.add_scalar(\"avg_reward\", avg_reward, total_timesteps)\n",
    "            writer.add_scalar(\"reward_step\", reward, total_timesteps)\n",
    "            writer.add_scalar(\"episode_reward\", episode_reward, total_timesteps)\n",
    "\n",
    "            if best_avg < avg_reward:\n",
    "                best_avg = avg_reward\n",
    "                print(\"saving best model....\\n\")\n",
    "                agent.save(\"lasttry\",\"TD3/saves\")\n",
    "\n",
    "            print(\"\\rTotal T: {:d} Episode Num: {:d} Reward: {:f} Avg Reward: {:f}\".format(\n",
    "                total_timesteps, episode_num, episode_reward, avg_reward), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "\n",
    "            if avg_reward >= REWARD_THRESH:\n",
    "                break\n",
    "\n",
    "            agent.train(replay_buffer, \n",
    "                        episode_timesteps, \n",
    "                        BATCH_SIZE, GAMMA, TAU, NOISE, NOISE_CLIP, POLICY_FREQUENCY)\n",
    "\n",
    "            episode_reward = 0\n",
    "            episode_timesteps = 0\n",
    "            episode_num += 1 \n",
    "\n",
    "        reward, done = runner.next_step(episode_timesteps)\n",
    "        episode_reward += reward\n",
    "\n",
    "        episode_timesteps += 1\n",
    "        total_timesteps += 1\n",
    "        timesteps_since_eval += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SEED = 5\n",
    "OBSERVATION = 10000\n",
    "EXPLORATION = 1000000\n",
    "BATCH_SIZE = 100\n",
    "GAMMA = 0.99\n",
    "TAU = 0.005\n",
    "NOISE = 0.2\n",
    "NOISE_CLIP = 0.5\n",
    "EXPLORE_NOISE = 0.2\n",
    "POLICY_FREQUENCY = 2\n",
    "EVAL_FREQUENCY = 5000\n",
    "REWARD_THRESH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HockeyEnv_BasicOpponent(weak_opponent=False)\n",
    "\n",
    "basic  = BasicOpponent(weak=False)\n",
    "td3   = TD3(env.observation_space, env.action_space, pretrained='stronger')\n",
    "loser = TD3(env.observation_space, env.action_space, pretrained='smallhall')\n",
    "q_agent = DQNAgent(env.observation_space, \n",
    "                         env.discrete_action_space,\n",
    "                        convert_func =  env.discrete_to_continous_action,\n",
    "                        pretrained   = 'DQN/weights/training_hall_1')\n",
    "\n",
    "#env.register_opponents([basic])#,ddpg,q_agent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "env.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "policy = TD3(env.observation_space, env.action_space, pretrained=False)\n",
    "\n",
    "replay_buffer = ReplayBuffer(max_size=300000)\n",
    "\n",
    "runner = Runner(env, policy, replay_buffer)\n",
    "\n",
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating Buffer 10000/10000."
     ]
    }
   ],
   "source": [
    "# Populate replay buffer\n",
    "observe(env, replay_buffer, OBSERVATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 23429 Episode Num: 139 Reward: -9.761105 Avg Reward: -5.23951951saving best model....\n",
      "\n",
      "Total T: 23482 Episode Num: 140 Reward: 25.098248 Avg Reward: -4.961365saving best model....\n",
      "\n",
      "Total T: 27543 Episode Num: 164 Reward: -0.361061 Avg Reward: -4.8943554saving best model....\n",
      "\n",
      "Total T: 27781 Episode Num: 166 Reward: -13.512818 Avg Reward: -4.666807saving best model....\n",
      "\n",
      "Total T: 28283 Episode Num: 168 Reward: -8.939579 Avg Reward: -4.5676238saving best model....\n",
      "\n",
      "Total T: 28534 Episode Num: 169 Reward: -1.852731 Avg Reward: -4.353128saving best model....\n",
      "\n",
      "Total T: 28584 Episode Num: 170 Reward: 24.126631 Avg Reward: -3.972783saving best model....\n",
      "\n",
      "Total T: 28618 Episode Num: 171 Reward: 24.881277 Avg Reward: -3.496415saving best model....\n",
      "\n",
      "Total T: 29198 Episode Num: 178 Reward: -13.449070 Avg Reward: -3.771616saving best model....\n",
      "\n",
      "Total T: 29455 Episode Num: 180 Reward: -13.023944 Avg Reward: -3.348651saving best model....\n",
      "\n",
      "Total T: 29539 Episode Num: 181 Reward: -9.749664 Avg Reward: -3.124280saving best model....\n",
      "\n",
      "Total T: 29747 Episode Num: 182 Reward: 32.525304 Avg Reward: -2.672970saving best model....\n",
      "\n",
      "Total T: 29998 Episode Num: 183 Reward: 10.737346 Avg Reward: -2.298066saving best model....\n",
      "\n",
      "Total T: 30242 Episode Num: 184 Reward: -2.816514 Avg Reward: -2.213964saving best model....\n",
      "\n",
      "Total T: 30362 Episode Num: 185 Reward: 26.498664 Avg Reward: -1.762585saving best model....\n",
      "\n",
      "Total T: 30613 Episode Num: 186 Reward: 5.834636 Avg Reward: -1.608561saving best model....\n",
      "\n",
      "Total T: 30650 Episode Num: 187 Reward: 24.915748 Avg Reward: -1.125283saving best model....\n",
      "\n",
      "Total T: 30901 Episode Num: 188 Reward: 0.844588 Avg Reward: -1.042220saving best model....\n",
      "\n",
      "Total T: 30945 Episode Num: 189 Reward: 25.524228 Avg Reward: -0.655209saving best model....\n",
      "\n",
      "Total T: 31196 Episode Num: 190 Reward: 5.520943 Avg Reward: -0.464152saving best model....\n",
      "\n",
      "Total T: 31265 Episode Num: 191 Reward: 24.813973 Avg Reward: -0.048229saving best model....\n",
      "\n",
      "Total T: 31516 Episode Num: 192 Reward: 4.455694 Avg Reward: 0.133000saving best model....\n",
      "\n",
      "Total T: 34718 Episode Num: 217 Reward: -0.872167 Avg Reward: -0.0402529saving best model....\n",
      "\n",
      "Total T: 34781 Episode Num: 218 Reward: 20.678077 Avg Reward: 0.191039saving best model....\n",
      "\n",
      "Total T: 41984 Episode Num: 278 Reward: -10.223841 Avg Reward: 0.2903919saving best model....\n",
      "\n",
      "Total T: 42125 Episode Num: 279 Reward: 20.628961 Avg Reward: 0.327219saving best model....\n",
      "\n",
      "Total T: 42155 Episode Num: 280 Reward: -12.526897 Avg Reward: 0.332190saving best model....\n",
      "\n",
      "Total T: 48531 Episode Num: 330 Reward: -12.779933 Avg Reward: 0.5155323saving best model....\n",
      "\n",
      "Total T: 48782 Episode Num: 331 Reward: 8.063456 Avg Reward: 0.700314saving best model....\n",
      "\n",
      "Total T: 48825 Episode Num: 332 Reward: 24.301740 Avg Reward: 1.066408saving best model....\n",
      "\n",
      "Total T: 48847 Episode Num: 333 Reward: 25.279700 Avg Reward: 1.067021saving best model....\n",
      "\n",
      "Total T: 49098 Episode Num: 334 Reward: -1.232736 Avg Reward: 1.219806saving best model....\n",
      "\n",
      "Total T: 49163 Episode Num: 335 Reward: 24.848213 Avg Reward: 1.564295saving best model....\n",
      "\n",
      "Total T: 49414 Episode Num: 336 Reward: -5.256170 Avg Reward: 1.679742saving best model....\n",
      "\n",
      "Total T: 49665 Episode Num: 337 Reward: 0.491281 Avg Reward: 1.706032saving best model....\n",
      "\n",
      "Total T: 49731 Episode Num: 338 Reward: -10.471498 Avg Reward: 1.749865saving best model....\n",
      "\n",
      "Total T: 49860 Episode Num: 340 Reward: -14.019569 Avg Reward: 1.906550saving best model....\n",
      "\n",
      "Total T: 50235 Episode Num: 344 Reward: 20.962406 Avg Reward: 2.1415726saving best model....\n",
      "\n",
      "Total T: 50263 Episode Num: 345 Reward: 25.948057 Avg Reward: 2.440432saving best model....\n",
      "\n",
      "Total T: 50593 Episode Num: 347 Reward: -10.325011 Avg Reward: 2.325802saving best model....\n",
      "\n",
      "Total T: 50685 Episode Num: 348 Reward: 23.811028 Avg Reward: 2.667189saving best model....\n",
      "\n",
      "Total T: 50713 Episode Num: 349 Reward: 25.806974 Avg Reward: 3.036842saving best model....\n",
      "\n",
      "Total T: 51357 Episode Num: 353 Reward: 5.349154 Avg Reward: 2.80662103saving best model....\n",
      "\n",
      "Total T: 51431 Episode Num: 354 Reward: 28.500079 Avg Reward: 3.199318saving best model....\n",
      "\n",
      "Total T: 91537 Episode Num: 661 Reward: 26.060468 Avg Reward: 3.1464287saving best model....\n",
      "\n",
      "Total T: 92915 Episode Num: 671 Reward: 6.173640 Avg Reward: 3.25538461saving best model....\n",
      "\n",
      "Total T: 94923 Episode Num: 681 Reward: 2.017924 Avg Reward: 3.50645122saving best model....\n",
      "\n",
      "Total T: 95174 Episode Num: 682 Reward: -2.113344 Avg Reward: 3.623821saving best model....\n",
      "\n",
      "Total T: 95197 Episode Num: 683 Reward: 25.348821 Avg Reward: 3.808046saving best model....\n",
      "\n",
      "Total T: 95299 Episode Num: 684 Reward: 29.500414 Avg Reward: 4.237028saving best model....\n",
      "\n",
      "Total T: 100901 Episode Num: 722 Reward: 24.628882 Avg Reward: 4.235645saving best model....\n",
      "\n",
      "Total T: 101011 Episode Num: 723 Reward: 23.661180 Avg Reward: 4.528684saving best model....\n",
      "\n",
      "Total T: 103126 Episode Num: 736 Reward: 21.721326 Avg Reward: 4.4306616saving best model....\n",
      "\n",
      "Total T: 103485 Episode Num: 738 Reward: 23.309944 Avg Reward: 4.584771saving best model....\n",
      "\n",
      "Total T: 103736 Episode Num: 739 Reward: 11.231580 Avg Reward: 4.675091saving best model....\n",
      "\n",
      "Total T: 103987 Episode Num: 740 Reward: 5.625200 Avg Reward: 4.714041saving best model....\n",
      "\n",
      "Total T: 104011 Episode Num: 741 Reward: 25.454155 Avg Reward: 4.916667saving best model....\n",
      "\n",
      "Total T: 104089 Episode Num: 742 Reward: 27.922785 Avg Reward: 5.293031saving best model....\n",
      "\n",
      "Total T: 104370 Episode Num: 744 Reward: -12.397029 Avg Reward: 5.207905saving best model....\n",
      "\n",
      "Total T: 104659 Episode Num: 749 Reward: 31.454901 Avg Reward: 5.4105667saving best model....\n",
      "\n",
      "Total T: 104773 Episode Num: 750 Reward: 22.496685 Avg Reward: 5.781963saving best model....\n",
      "\n",
      "Total T: 111806 Episode Num: 804 Reward: -8.872708 Avg Reward: 5.7146612saving best model....\n",
      "\n",
      "Total T: 112057 Episode Num: 805 Reward: 14.608507 Avg Reward: 5.985551saving best model....\n",
      "\n",
      "Total T: 112157 Episode Num: 806 Reward: 24.623711 Avg Reward: 6.340886saving best model....\n",
      "\n",
      "Total T: 112338 Episode Num: 807 Reward: 37.279658 Avg Reward: 6.683014saving best model....\n",
      "\n",
      "Total T: 112650 Episode Num: 810 Reward: -12.628505 Avg Reward: 6.466720saving best model....\n",
      "\n",
      "Total T: 112828 Episode Num: 812 Reward: -9.642366 Avg Reward: 6.793121saving best model....\n",
      "\n",
      "Total T: 115025 Episode Num: 830 Reward: 24.778340 Avg Reward: 7.0272444saving best model....\n",
      "\n",
      "Total T: 146177 Episode Num: 1062 Reward: 24.159204 Avg Reward: 7.1670517saving best model....\n",
      "\n",
      "Total T: 146237 Episode Num: 1063 Reward: 27.454005 Avg Reward: 7.536175saving best model....\n",
      "\n",
      "Total T: 146488 Episode Num: 1064 Reward: 6.727574 Avg Reward: 7.656298saving best model....\n",
      "\n",
      "Total T: 146652 Episode Num: 1065 Reward: 32.776912 Avg Reward: 7.713897saving best model....\n",
      "\n",
      "Total T: 146697 Episode Num: 1066 Reward: 24.495952 Avg Reward: 7.713901saving best model....\n",
      "\n",
      "Total T: 147444 Episode Num: 1072 Reward: 24.502040 Avg Reward: 7.8168442saving best model....\n",
      "\n",
      "Total T: 150395 Episode Num: 1093 Reward: 3.871142 Avg Reward: 7.86559694saving best model....\n",
      "\n",
      "Total T: 150791 Episode Num: 1096 Reward: -17.932252 Avg Reward: 8.163375saving best model....\n",
      "\n",
      "Total T: 150819 Episode Num: 1097 Reward: 25.818783 Avg Reward: 8.421889saving best model....\n",
      "\n",
      "Total T: 150858 Episode Num: 1098 Reward: -12.923520 Avg Reward: 8.428525saving best model....\n",
      "\n",
      "Total T: 150955 Episode Num: 1100 Reward: -12.890282 Avg Reward: 8.439085saving best model....\n",
      "\n",
      "Total T: 153847 Episode Num: 1124 Reward: 24.604312 Avg Reward: 8.5457222saving best model....\n",
      "\n",
      "Total T: 154098 Episode Num: 1125 Reward: 11.552979 Avg Reward: 8.739632saving best model....\n",
      "\n",
      "Total T: 155178 Episode Num: 1132 Reward: -13.217734 Avg Reward: 9.050488saving best model....\n",
      "\n",
      "Total T: 155222 Episode Num: 1133 Reward: 26.378332 Avg Reward: 9.339195saving best model....\n",
      "\n",
      "Total T: 155257 Episode Num: 1134 Reward: -12.563158 Avg Reward: 9.360328saving best model....\n",
      "\n",
      "Total T: 167537 Episode Num: 1233 Reward: 28.014662 Avg Reward: 9.3328582saving best model....\n",
      "\n",
      "Total T: 168421 Episode Num: 1240 Reward: -5.074610 Avg Reward: 9.6515168saving best model....\n",
      "\n",
      "Total T: 168445 Episode Num: 1241 Reward: 25.755479 Avg Reward: 9.840608saving best model....\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 168663 Episode Num: 1242 Reward: 20.195531 Avg Reward: 10.139022saving best model....\n",
      "\n",
      "Total T: 168694 Episode Num: 1243 Reward: 24.764423 Avg Reward: 10.446942saving best model....\n",
      "\n",
      "Total T: 168786 Episode Num: 1244 Reward: 23.787068 Avg Reward: 10.811367saving best model....\n",
      "\n",
      "Total T: 168853 Episode Num: 1245 Reward: 27.498815 Avg Reward: 10.825773saving best model....\n",
      "\n",
      "Total T: 168911 Episode Num: 1246 Reward: 24.524173 Avg Reward: 10.922100saving best model....\n",
      "\n",
      "Total T: 168941 Episode Num: 1247 Reward: 25.614533 Avg Reward: 11.163769saving best model....\n",
      "\n",
      "Total T: 168986 Episode Num: 1248 Reward: 24.465771 Avg Reward: 11.540386saving best model....\n",
      "\n",
      "Total T: 169395 Episode Num: 1250 Reward: -3.781459 Avg Reward: 11.536050saving best model....\n",
      "\n",
      "Total T: 169449 Episode Num: 1251 Reward: 29.121929 Avg Reward: 11.794942saving best model....\n",
      "\n",
      "Total T: 169591 Episode Num: 1252 Reward: 26.373674 Avg Reward: 11.832285saving best model....\n",
      "\n",
      "Total T: 169901 Episode Num: 1254 Reward: 2.685004 Avg Reward: 12.1467440saving best model....\n",
      "\n",
      "Total T: 170058 Episode Num: 1256 Reward: -12.094267 Avg Reward: 12.395947saving best model....\n",
      "\n",
      "Total T: 170084 Episode Num: 1257 Reward: 25.557554 Avg Reward: 12.665245saving best model....\n",
      "\n",
      "Total T: 170403 Episode Num: 1259 Reward: 26.955381 Avg Reward: 12.825181saving best model....\n",
      "\n",
      "Total T: 170654 Episode Num: 1260 Reward: 11.043444 Avg Reward: 13.074645saving best model....\n",
      "\n",
      "Total T: 171120 Episode Num: 1263 Reward: -6.708654 Avg Reward: 12.865445saving best model....\n",
      "\n",
      "Total T: 171511 Episode Num: 1266 Reward: -10.569094 Avg Reward: 12.941744saving best model....\n",
      "\n",
      "Total T: 171694 Episode Num: 1269 Reward: 26.222811 Avg Reward: 13.257997saving best model....\n",
      "\n",
      "Total T: 212169 Episode Num: 1591 Reward: 25.708534 Avg Reward: 13.2158634saving best model....\n",
      "\n",
      "Total T: 212246 Episode Num: 1593 Reward: 25.773290 Avg Reward: 13.501152saving best model....\n",
      "\n",
      "Total T: 212439 Episode Num: 1594 Reward: 28.615386 Avg Reward: 13.819427saving best model....\n",
      "\n",
      "Total T: 213255 Episode Num: 1599 Reward: 25.992677 Avg Reward: 13.787695saving best model....\n",
      "\n",
      "Total T: 213313 Episode Num: 1600 Reward: 24.727360 Avg Reward: 14.167559saving best model....\n",
      "\n",
      "Total T: 213375 Episode Num: 1601 Reward: 28.182263 Avg Reward: 14.201607saving best model....\n",
      "\n",
      "Total T: 213490 Episode Num: 1602 Reward: 29.634761 Avg Reward: 14.543563saving best model....\n",
      "\n",
      "Total T: 213738 Episode Num: 1604 Reward: -10.262148 Avg Reward: 14.543512saving best model....\n",
      "\n",
      "Total T: 213947 Episode Num: 1605 Reward: 37.907451 Avg Reward: 14.666358saving best model....\n",
      "\n",
      "Total T: 214002 Episode Num: 1606 Reward: 24.584291 Avg Reward: 14.669026saving best model....\n",
      "\n",
      "Total T: 214200 Episode Num: 1607 Reward: 25.957586 Avg Reward: 14.876807saving best model....\n",
      "\n",
      "Total T: 214246 Episode Num: 1608 Reward: 24.496653 Avg Reward: 14.899194saving best model....\n",
      "\n",
      "Total T: 218500 Episode Num: 1641 Reward: 38.975555 Avg Reward: 14.7942886saving best model....\n",
      "\n",
      "Total T: 253385 Episode Num: 1917 Reward: 25.776737 Avg Reward: 14.8472419saving best model....\n",
      "\n",
      "Total T: 267615 Episode Num: 2038 Reward: -6.182190 Avg Reward: 14.8954822saving best model....\n",
      "\n",
      "Total T: 271957 Episode Num: 2074 Reward: 24.709278 Avg Reward: 15.1257160saving best model....\n",
      "\n",
      "Total T: 272261 Episode Num: 2076 Reward: -3.243106 Avg Reward: 15.240571saving best model....\n",
      "\n",
      "Total T: 272555 Episode Num: 2078 Reward: -6.580088 Avg Reward: 15.409089saving best model....\n",
      "\n",
      "Total T: 273997 Episode Num: 2095 Reward: 26.943352 Avg Reward: 15.5203840saving best model....\n",
      "\n",
      "Total T: 274181 Episode Num: 2096 Reward: 32.735793 Avg Reward: 15.970410saving best model....\n",
      "\n",
      "Total T: 274405 Episode Num: 2097 Reward: 35.833169 Avg Reward: 16.429951saving best model....\n",
      "\n",
      "Total T: 274823 Episode Num: 2102 Reward: 32.476841 Avg Reward: 16.3893430saving best model....\n",
      "\n",
      "Total T: 274858 Episode Num: 2103 Reward: 25.551716 Avg Reward: 16.549951saving best model....\n",
      "\n",
      "Total T: 298159 Episode Num: 2311 Reward: 27.081675 Avg Reward: 16.3461381saving best model....\n",
      "\n",
      "Total T: 298214 Episode Num: 2312 Reward: 24.672772 Avg Reward: 16.699463saving best model....\n",
      "\n",
      "Total T: 298322 Episode Num: 2314 Reward: -12.682889 Avg Reward: 16.703054saving best model....\n",
      "\n",
      "Total T: 298655 Episode Num: 2317 Reward: 6.442651 Avg Reward: 16.8591761saving best model....\n",
      "\n",
      "Total T: 298750 Episode Num: 2318 Reward: 23.840941 Avg Reward: 17.119478saving best model....\n",
      "\n",
      "Total T: 299034 Episode Num: 2320 Reward: 1.109104 Avg Reward: 17.0925783saving best model....\n",
      "\n",
      "Total T: 299167 Episode Num: 2322 Reward: -12.513869 Avg Reward: 17.441209saving best model....\n",
      "\n",
      "Total T: 299232 Episode Num: 2323 Reward: 27.896384 Avg Reward: 17.785549saving best model....\n",
      "\n",
      "Total T: 299479 Episode Num: 2327 Reward: 29.441807 Avg Reward: 17.835787saving best model....\n",
      "\n",
      "Total T: 300478 Episode Num: 2337 Reward: 25.955515 Avg Reward: 18.1988840saving best model....\n",
      "\n",
      "Total T: 300663 Episode Num: 2338 Reward: -2.110496 Avg Reward: 18.317070saving best model....\n",
      "\n",
      "Total T: 301450 Episode Num: 2348 Reward: 27.701635 Avg Reward: 18.1112540saving best model....\n",
      "\n",
      "Total T: 301655 Episode Num: 2349 Reward: 31.621403 Avg Reward: 18.480616saving best model....\n",
      "\n",
      "Total T: 310940 Episode Num: 2433 Reward: 25.627162 Avg Reward: 18.7051291saving best model....\n",
      "\n",
      "Total T: 348414 Episode Num: 2809 Reward: 28.065760 Avg Reward: 19.0021708saving best model....\n",
      "\n",
      "Total T: 349342 Episode Num: 2818 Reward: 27.544769 Avg Reward: 19.286883saving best model....\n",
      "\n",
      "Total T: 354803 Episode Num: 2867 Reward: 27.918507 Avg Reward: 19.0327209saving best model....\n",
      "\n",
      "Total T: 354897 Episode Num: 2868 Reward: 24.163073 Avg Reward: 19.412761saving best model....\n",
      "\n",
      "Total T: 375287 Episode Num: 3087 Reward: 25.064828 Avg Reward: 19.3924730saving best model....\n",
      "\n",
      "Total T: 375526 Episode Num: 3089 Reward: 25.768797 Avg Reward: 19.730340saving best model....\n",
      "\n",
      "Total T: 375625 Episode Num: 3090 Reward: 24.099845 Avg Reward: 20.095203saving best model....\n",
      "\n",
      "Total T: 377753 Episode Num: 3112 Reward: 28.304557 Avg Reward: 20.416852saving best model....\n",
      "\n",
      "Total T: 377960 Episode Num: 3113 Reward: 36.766283 Avg Reward: 20.525229saving best model....\n",
      "\n",
      "Total T: 378109 Episode Num: 3115 Reward: 25.993128 Avg Reward: 20.548825saving best model....\n",
      "\n",
      "Total T: 378157 Episode Num: 3116 Reward: 24.411267 Avg Reward: 20.559158saving best model....\n",
      "\n",
      "Total T: 378202 Episode Num: 3117 Reward: 25.193691 Avg Reward: 20.725985saving best model....\n",
      "\n",
      "Total T: 378372 Episode Num: 3120 Reward: 24.622589 Avg Reward: 20.738613saving best model....\n",
      "\n",
      "Total T: 378406 Episode Num: 3121 Reward: 25.611865 Avg Reward: 21.079350saving best model....\n",
      "\n",
      "Total T: 378491 Episode Num: 3122 Reward: 26.602813 Avg Reward: 21.098478saving best model....\n",
      "\n",
      "Total T: 378718 Episode Num: 3125 Reward: 25.921349 Avg Reward: 21.082226saving best model....\n",
      "\n",
      "Total T: 378814 Episode Num: 3126 Reward: 26.811849 Avg Reward: 21.107496saving best model....\n",
      "\n",
      "Total T: 378885 Episode Num: 3127 Reward: 30.409809 Avg Reward: 21.130755saving best model....\n",
      "\n",
      "Total T: 379855 Episode Num: 3137 Reward: -3.138225 Avg Reward: 20.788990saving best model....\n",
      "\n",
      "Total T: 379956 Episode Num: 3138 Reward: 26.791217 Avg Reward: 21.184035saving best model....\n",
      "\n",
      "Total T: 380967 Episode Num: 3147 Reward: 29.553398 Avg Reward: 21.331375saving best model....\n",
      "\n",
      "Total T: 381967 Episode Num: 3159 Reward: 25.823286 Avg Reward: 21.5295422saving best model....\n",
      "\n",
      "Total T: 382019 Episode Num: 3160 Reward: 24.614602 Avg Reward: 21.795192saving best model....\n",
      "\n",
      "Total T: 382419 Episode Num: 3162 Reward: 2.135497 Avg Reward: 21.6696444saving best model....\n",
      "\n",
      "Total T: 382453 Episode Num: 3163 Reward: 25.720839 Avg Reward: 21.967681saving best model....\n",
      "\n",
      "Total T: 382577 Episode Num: 3165 Reward: 25.923447 Avg Reward: 21.950957saving best model....\n",
      "\n",
      "Total T: 382673 Episode Num: 3166 Reward: 27.635873 Avg Reward: 22.155518saving best model....\n",
      "\n",
      "Total T: 382707 Episode Num: 3167 Reward: 25.925090 Avg Reward: 22.499878saving best model....\n",
      "\n",
      "Total T: 382822 Episode Num: 3168 Reward: 26.303743 Avg Reward: 22.883682saving best model....\n",
      "\n",
      "Total T: 403463 Episode Num: 3394 Reward: 23.981818 Avg Reward: 22.5669615saving best model....\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 403502 Episode Num: 3395 Reward: 25.637483 Avg Reward: 22.914925saving best model....\n",
      "\n",
      "Total T: 403549 Episode Num: 3396 Reward: 24.523169 Avg Reward: 23.149955saving best model....\n",
      "\n",
      "Total T: 460423 Episode Num: 4183 Reward: 25.926721 Avg Reward: 23.0259954saving best model....\n",
      "\n",
      "Total T: 460466 Episode Num: 4184 Reward: 24.627376 Avg Reward: 23.237855saving best model....\n",
      "\n",
      "Total T: 460502 Episode Num: 4185 Reward: 26.297546 Avg Reward: 23.244775saving best model....\n",
      "\n",
      "Total T: 468793 Episode Num: 4312 Reward: 27.464144 Avg Reward: 23.6493561saving best model....\n",
      "\n",
      "Total T: 469518 Episode Num: 4325 Reward: 25.969783 Avg Reward: 23.5447709saving best model....\n",
      "\n",
      "Total T: 469777 Episode Num: 4331 Reward: 26.414117 Avg Reward: 23.854595saving best model....\n",
      "\n",
      "Total T: 473909 Episode Num: 4396 Reward: 24.435295 Avg Reward: 21.6095877"
     ]
    }
   ],
   "source": [
    "# Train agent\n",
    "train(policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.save(filename=\"last\", directory=\"TD3/saves\")\n",
    "\n",
    "evaluate_policy(policy, env, eval_episodes=100, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.update_weights(add=True)\n",
    "stats = np.array(env.weights).T\n",
    "x,y = stats.shape\n",
    "plt.figure(figsize=(16,5))\n",
    "for i,agent,name in zip(stats,env.agents,['weak','strong']):\n",
    "    plt.plot(np.arange(y-1)*50,i[1:],label=name)\n",
    "plt.legend()\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Winscore')\n",
    "plt.savefig('TD3/Plots/last.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

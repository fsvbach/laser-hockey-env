{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twin Delayed Deep Deterministic Policy Gradient (TD3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "from laserhockey.hockey_env import HockeyEnv_BasicOpponent\n",
    "import gym\n",
    "#import roboschool\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Initialize parameters and build model.\n",
    "        Args:\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            max_action (float): highest action to take\n",
    "            seed (int): Random seed\n",
    "            h1_units (int): Number of nodes in first hidden layer\n",
    "            h2_units (int): Number of nodes in second hidden layer\n",
    "            \n",
    "        Return:\n",
    "            action output of network with tanh activation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, action_dim)\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.max_action * torch.tanh(self.l3(x)) \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Initialize parameters and build model.\n",
    "        Args:\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            max_action (float): highest action to take\n",
    "            seed (int): Random seed\n",
    "            h1_units (int): Number of nodes in first hidden layer\n",
    "            h2_units (int): Number of nodes in second hidden layer\n",
    "            \n",
    "        Return:\n",
    "            value output of network \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.l4 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.l5 = nn.Linear(400, 300)\n",
    "        self.l6 = nn.Linear(300, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "\n",
    "        x1 = F.relu(self.l1(xu))\n",
    "        x1 = F.relu(self.l2(x1))\n",
    "        x1 = self.l3(x1)\n",
    "\n",
    "        x2 = F.relu(self.l4(xu))\n",
    "        x2 = F.relu(self.l5(x2))\n",
    "        x2 = self.l6(x2)\n",
    "        return x1, x2\n",
    "\n",
    "\n",
    "    def Q1(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "\n",
    "        x1 = F.relu(self.l1(xu))\n",
    "        x1 = F.relu(self.l2(x1))\n",
    "        x1 = self.l3(x1)\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code based on: \n",
    "# https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
    "\n",
    "# Expects tuples of (state, next_state, action, reward, done)\n",
    "class ReplayBuffer(object):\n",
    "    \"\"\"Buffer to store tuples of experience replay\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size=1000000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_size (int): total amount of tuples to store\n",
    "        \"\"\"\n",
    "        \n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    def add(self, data):\n",
    "        \"\"\"Add experience tuples to buffer\n",
    "        \n",
    "        Args:\n",
    "            data (tuple): experience replay tuple\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = data\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Samples a random amount of experiences from buffer of batch size\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): size of sample\n",
    "        \"\"\"\n",
    "        \n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        states, actions, next_states, rewards, dones = [], [], [], [], []\n",
    "\n",
    "        for i in ind: \n",
    "            s, a, s_, r, d = self.storage[i]\n",
    "            states.append(np.array(s, copy=False))\n",
    "            actions.append(np.array(a, copy=False))\n",
    "            next_states.append(np.array(s_, copy=False))\n",
    "            rewards.append(np.array(r, copy=False))\n",
    "            dones.append(np.array(d, copy=False))\n",
    "\n",
    "        return np.array(states), np.array(actions), np.array(next_states), np.array(rewards).reshape(-1, 1), np.array(dones).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(object):\n",
    "    \"\"\"Agent class that handles the training of the networks and provides outputs as actions\n",
    "    \n",
    "        Args:\n",
    "            state_dim (int): state size\n",
    "            action_dim (int): action size\n",
    "            max_action (float): highest action to take\n",
    "            device (device): cuda or cpu to process tensors\n",
    "            env (env): gym environment to use\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, max_action, env):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-3)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.env = env\n",
    "\n",
    "\n",
    "        \n",
    "    def select_action(self, state, noise=0.1):\n",
    "        \"\"\"Select an appropriate action from the agent policy\n",
    "        \n",
    "            Args:\n",
    "                state (array): current state of environment\n",
    "                noise (float): how much noise to add to acitons\n",
    "                \n",
    "            Returns:\n",
    "                action (float): action clipped within action range\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        \n",
    "        action = self.actor(state).cpu().data.numpy().flatten()\n",
    "        if noise != 0: \n",
    "            action = (action + np.random.normal(0, noise, size=self.env.action_space.shape[0]))\n",
    "            \n",
    "        return action.clip(self.env.action_space.low, self.env.action_space.high)\n",
    "\n",
    "    \n",
    "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "        \"\"\"Train and update actor and critic networks\n",
    "        \n",
    "            Args:\n",
    "                replay_buffer (ReplayBuffer): buffer for experience replay\n",
    "                iterations (int): how many times to run training\n",
    "                batch_size(int): batch size to sample from replay buffer\n",
    "                discount (float): discount factor\n",
    "                tau (float): soft update for main networks to target networks\n",
    "                \n",
    "            Return:\n",
    "                actor_loss (float): loss from actor network\n",
    "                critic_loss (float): loss from critic network\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        for it in range(iterations):\n",
    "\n",
    "            # Sample replay buffer \n",
    "            x, y, u, r, d = replay_buffer.sample(batch_size)\n",
    "            state = torch.FloatTensor(x).to(device)\n",
    "            action = torch.FloatTensor(u).to(device)\n",
    "            next_state = torch.FloatTensor(y).to(device)\n",
    "            done = torch.FloatTensor(1 - d).to(device)\n",
    "            reward = torch.FloatTensor(r).to(device)\n",
    "\n",
    "            # Select action according to policy and add clipped noise \n",
    "            noise = torch.FloatTensor(u).data.normal_(0, policy_noise).to(device)\n",
    "            noise = noise.clamp(-noise_clip, noise_clip)\n",
    "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            # Compute the target Q value\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + (done * discount * target_Q).detach()\n",
    "\n",
    "            # Get current Q estimates\n",
    "            current_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "            # Compute critic loss\n",
    "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q) \n",
    "\n",
    "            # Optimize the critic\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            # Delayed policy updates\n",
    "            if it % policy_freq == 0:\n",
    "\n",
    "                # Compute actor loss\n",
    "                actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "\n",
    "                # Optimize the actor \n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                # Update the frozen target models\n",
    "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "\n",
    "    def save(self, filename, directory):\n",
    "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "\n",
    "\n",
    "    def load(self, filename=\"best_avg\", directory=\"TD3/saves\"):\n",
    "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner():\n",
    "    \"\"\"Carries out the environment steps and adds experiences to memory\"\"\"\n",
    "    \n",
    "    def __init__(self, env, agent, replay_buffer):\n",
    "        \n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.obs = env.reset()\n",
    "        self.done = False\n",
    "        \n",
    "    def next_step(self, episode_timesteps, noise=0.1):\n",
    "        \n",
    "        action = self.agent.select_action(np.array(self.obs), noise=0.1)\n",
    "\n",
    "        # Perform action\n",
    "        new_obs, reward, done, info = self.env.step(action) \n",
    "        done_bool = 0 if episode_timesteps + 1 == 250 else float(done)\n",
    "        \n",
    "        #add proxy\n",
    "        reward += sum(info.values())\n",
    "        \n",
    "        # Store data in replay buffer\n",
    "        replay_buffer.add((self.obs, new_obs, action, reward, done_bool))\n",
    "        \n",
    "        self.obs = new_obs\n",
    "        \n",
    "        if done:\n",
    "            self.obs = self.env.reset()\n",
    "            done = False\n",
    "            \n",
    "            return reward, True\n",
    "        \n",
    "        return reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, env, eval_episodes=100,render=False):\n",
    "    \"\"\"run several episodes using the best agent policy\n",
    "        \n",
    "        Args:\n",
    "            policy (agent): agent to evaluate\n",
    "            env (env): gym environment\n",
    "            eval_episodes (int): how many test episodes to run\n",
    "            render (bool): show training\n",
    "        \n",
    "        Returns:\n",
    "            avg_reward (float): average reward over the number of evaluations\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    avg_reward = 0.\n",
    "    for i in range(eval_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = policy.select_action(np.array(obs), noise=0)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "\n",
    "    print(\"\\n---------------------------------------\")\n",
    "    print(\"Evaluation over {:d} episodes: {:f}\" .format(eval_episodes, avg_reward))\n",
    "    print(\"---------------------------------------\")\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe(env,replay_buffer, observation_steps):\n",
    "    \"\"\"run episodes while taking random actions and filling replay_buffer\n",
    "    \n",
    "        Args:\n",
    "            env (env): gym environment\n",
    "            replay_buffer(ReplayBuffer): buffer to store experience replay\n",
    "            observation_steps (int): how many steps to observe for\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    time_steps = 0\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while time_steps < observation_steps:\n",
    "        action = env.action_space.sample()\n",
    "        new_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        replay_buffer.add((obs, new_obs, action, reward, done))\n",
    "\n",
    "        obs = new_obs\n",
    "        time_steps += 1\n",
    "\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "\n",
    "        print(\"\\rPopulating Buffer {}/{}.\".format(time_steps, observation_steps), end=\"\")\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, test_env):\n",
    "    \"\"\"Train the agent for exploration steps\n",
    "    \n",
    "        Args:\n",
    "            agent (Agent): agent to use\n",
    "            env (environment): gym environment\n",
    "            writer (SummaryWriter): tensorboard writer\n",
    "            exploration (int): how many training steps to run\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    total_timesteps = 0\n",
    "    timesteps_since_eval = 0\n",
    "    episode_num = 0\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    done = False \n",
    "    obs = env.reset()\n",
    "    evaluations = []\n",
    "    rewards = []\n",
    "    best_avg = -2000\n",
    "    \n",
    "    writer = SummaryWriter(logdir='TD3/runs/CURRENT_DATETIME_HOSTNAME', comment=\"-basicopponent\")\n",
    "    \n",
    "    while total_timesteps < EXPLORATION:\n",
    "    \n",
    "        if done: \n",
    "\n",
    "            if total_timesteps != 0: \n",
    "                rewards.append(episode_reward)\n",
    "                avg_reward = np.mean(rewards[-100:])\n",
    "                \n",
    "                writer.add_scalar(\"avg_reward\", avg_reward, total_timesteps)\n",
    "                writer.add_scalar(\"reward_step\", reward, total_timesteps)\n",
    "                writer.add_scalar(\"episode_reward\", episode_reward, total_timesteps)\n",
    "                \n",
    "                if best_avg < avg_reward:\n",
    "                    best_avg = avg_reward\n",
    "                    print(\"saving best model....\\n\")\n",
    "                    agent.save(\"best_avg\",\"TD3/saves\")\n",
    "\n",
    "                print(\"\\rTotal T: {:d} Episode Num: {:d} Reward: {:f} Avg Reward: {:f}\".format(\n",
    "                    total_timesteps, episode_num, episode_reward, avg_reward), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "\n",
    "                if avg_reward >= REWARD_THRESH:\n",
    "                    break\n",
    "\n",
    "                agent.train(replay_buffer, episode_timesteps, BATCH_SIZE, GAMMA, TAU, NOISE, NOISE_CLIP, POLICY_FREQUENCY)\n",
    "\n",
    "                # Evaluate episode\n",
    "#                 if timesteps_since_eval >= EVAL_FREQUENCY:\n",
    "#                     timesteps_since_eval %= EVAL_FREQUENCY\n",
    "#                     eval_reward = evaluate_policy(agent, test_env)\n",
    "#                     evaluations.append(avg_reward)\n",
    "#                     writer.add_scalar(\"eval_reward\", eval_reward, total_timesteps)\n",
    "\n",
    "#                     if best_avg < eval_reward:\n",
    "#                         best_avg = eval_reward\n",
    "#                         print(\"saving best model....\\n\")\n",
    "#                         agent.save(\"best_avg\",\"saves\")\n",
    "\n",
    "                episode_reward = 0\n",
    "                episode_timesteps = 0\n",
    "                episode_num += 1 \n",
    "\n",
    "        reward, done = runner.next_step(episode_timesteps)\n",
    "        episode_reward += reward\n",
    "\n",
    "        episode_timesteps += 1\n",
    "        total_timesteps += 1\n",
    "        timesteps_since_eval += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HockeyEnv_BasicOpponent(weak_opponent=False)\n",
    "SEED = 0\n",
    "OBSERVATION = 10000\n",
    "EXPLORATION = 50000\n",
    "BATCH_SIZE = 100\n",
    "GAMMA = 0.99\n",
    "TAU = 0.005\n",
    "NOISE = 0.2\n",
    "NOISE_CLIP = 0.5\n",
    "EXPLORE_NOISE = 0.1\n",
    "POLICY_FREQUENCY = 2\n",
    "EVAL_FREQUENCY = 5000\n",
    "REWARD_THRESH = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set seeds\n",
    "env.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "policy = TD3(state_dim, action_dim, max_action, env)\n",
    "\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "runner = Runner(env, policy, replay_buffer)\n",
    "\n",
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating Buffer 10000/10000."
     ]
    }
   ],
   "source": [
    "# Populate replay buffer\n",
    "observe(env, replay_buffer, OBSERVATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving best model....\n",
      "\n",
      "Total T: 13843 Episode Num: 83 Reward: -21.362847 Avg Reward: -18.513904saving best model....\n",
      "\n",
      "Total T: 14345 Episode Num: 85 Reward: -28.363944 Avg Reward: -18.412019saving best model....\n",
      "\n",
      "Total T: 14847 Episode Num: 87 Reward: -23.219783 Avg Reward: -18.262187saving best model....\n",
      "\n",
      "Total T: 15098 Episode Num: 88 Reward: -2.448473 Avg Reward: -18.084505saving best model....\n",
      "\n",
      "Total T: 15349 Episode Num: 89 Reward: -9.709316 Avg Reward: -17.991448saving best model....\n",
      "\n",
      "Total T: 15372 Episode Num: 90 Reward: -12.640661 Avg Reward: -17.932648saving best model....\n",
      "\n",
      "Total T: 15430 Episode Num: 91 Reward: -14.067720 Avg Reward: -17.890638saving best model....\n",
      "\n",
      "Total T: 16146 Episode Num: 95 Reward: -9.600776 Avg Reward: -17.8557782saving best model....\n",
      "\n",
      "Total T: 16550 Episode Num: 98 Reward: 9.349495 Avg Reward: -17.82783454saving best model....\n",
      "\n",
      "Total T: 16587 Episode Num: 99 Reward: 10.379598 Avg Reward: -17.545760saving best model....\n",
      "\n",
      "Total T: 16838 Episode Num: 100 Reward: -2.143259 Avg Reward: -17.383185saving best model....\n",
      "\n",
      "Total T: 17164 Episode Num: 102 Reward: -7.541612 Avg Reward: -16.872543saving best model....\n",
      "\n",
      "Total T: 17897 Episode Num: 106 Reward: -24.182601 Avg Reward: -16.947359saving best model....\n",
      "\n",
      "Total T: 18560 Episode Num: 110 Reward: -14.946648 Avg Reward: -16.756455saving best model....\n",
      "\n",
      "Total T: 18811 Episode Num: 111 Reward: -21.326859 Avg Reward: -16.506309saving best model....\n",
      "\n",
      "Total T: 18956 Episode Num: 112 Reward: -16.996971 Avg Reward: -16.392190saving best model....\n",
      "\n",
      "Total T: 19136 Episode Num: 113 Reward: 4.000296 Avg Reward: -15.934601saving best model....\n",
      "\n",
      "Total T: 19387 Episode Num: 114 Reward: -10.141348 Avg Reward: -15.865625saving best model....\n",
      "\n",
      "Total T: 19436 Episode Num: 115 Reward: -12.095379 Avg Reward: -15.854785saving best model....\n",
      "\n",
      "Total T: 19477 Episode Num: 116 Reward: -12.435074 Avg Reward: -15.718988saving best model....\n",
      "\n",
      "Total T: 19728 Episode Num: 117 Reward: -14.504916 Avg Reward: -15.357180saving best model....\n",
      "\n",
      "Total T: 19979 Episode Num: 118 Reward: -2.342061 Avg Reward: -15.204079saving best model....\n",
      "\n",
      "Total T: 20211 Episode Num: 120 Reward: -14.225934 Avg Reward: -14.981547saving best model....\n",
      "\n",
      "Total T: 20370 Episode Num: 122 Reward: -14.637382 Avg Reward: -14.777292saving best model....\n",
      "\n",
      "Total T: 20421 Episode Num: 123 Reward: 9.052815 Avg Reward: -14.330019saving best model....\n",
      "\n",
      "Total T: 20672 Episode Num: 124 Reward: -6.632265 Avg Reward: -14.239010saving best model....\n",
      "\n",
      "Total T: 20923 Episode Num: 125 Reward: -3.300207 Avg Reward: -13.947590saving best model....\n",
      "\n",
      "Total T: 21174 Episode Num: 126 Reward: 0.153113 Avg Reward: -13.807067saving best model....\n",
      "\n",
      "Total T: 21200 Episode Num: 127 Reward: 10.919039 Avg Reward: -13.483094saving best model....\n",
      "\n",
      "Total T: 21451 Episode Num: 128 Reward: -3.140039 Avg Reward: -13.390653saving best model....\n",
      "\n",
      "Total T: 22679 Episode Num: 136 Reward: -14.149714 Avg Reward: -13.348298saving best model....\n",
      "\n",
      "Total T: 22788 Episode Num: 138 Reward: -13.409268 Avg Reward: -13.229756saving best model....\n",
      "\n",
      "Total T: 22905 Episode Num: 140 Reward: -14.687651 Avg Reward: -12.771579saving best model....\n",
      "\n",
      "Total T: 23020 Episode Num: 141 Reward: -11.508398 Avg Reward: -12.518123saving best model....\n",
      "\n",
      "Total T: 23081 Episode Num: 142 Reward: 11.113114 Avg Reward: -12.262352saving best model....\n",
      "\n",
      "Total T: 23246 Episode Num: 144 Reward: -14.854564 Avg Reward: -12.225646saving best model....\n",
      "\n",
      "Total T: 23387 Episode Num: 146 Reward: -15.511134 Avg Reward: -11.824714saving best model....\n",
      "\n",
      "Total T: 23601 Episode Num: 148 Reward: -12.876290 Avg Reward: -11.805440saving best model....\n",
      "\n",
      "Total T: 24014 Episode Num: 151 Reward: -12.944639 Avg Reward: -11.784175saving best model....\n",
      "\n",
      "Total T: 24265 Episode Num: 152 Reward: 4.355011 Avg Reward: -11.529408saving best model....\n",
      "\n",
      "Total T: 24595 Episode Num: 154 Reward: -15.348555 Avg Reward: -11.128507saving best model....\n",
      "\n",
      "Total T: 24650 Episode Num: 155 Reward: 10.539381 Avg Reward: -10.823160saving best model....\n",
      "\n",
      "Total T: 25236 Episode Num: 158 Reward: -18.614029 Avg Reward: -10.806616saving best model....\n",
      "\n",
      "Total T: 25532 Episode Num: 160 Reward: -12.442146 Avg Reward: -10.556419saving best model....\n",
      "\n",
      "Total T: 25951 Episode Num: 165 Reward: 11.582912 Avg Reward: -10.2733479saving best model....\n",
      "\n",
      "Total T: 26202 Episode Num: 166 Reward: -2.767600 Avg Reward: -10.139408saving best model....\n",
      "\n",
      "Total T: 26351 Episode Num: 167 Reward: 6.249769 Avg Reward: -9.826382saving best model....\n",
      "\n",
      "Total T: 26449 Episode Num: 168 Reward: 11.898071 Avg Reward: -9.571128saving best model....\n",
      "\n",
      "Total T: 26539 Episode Num: 169 Reward: 8.173453 Avg Reward: -9.341226saving best model....\n",
      "\n",
      "Total T: 26727 Episode Num: 171 Reward: -13.674940 Avg Reward: -9.202735saving best model....\n",
      "\n",
      "Total T: 26978 Episode Num: 172 Reward: -1.289781 Avg Reward: -9.041871saving best model....\n",
      "\n",
      "Total T: 27089 Episode Num: 173 Reward: 13.264591 Avg Reward: -8.676264saving best model....\n",
      "\n",
      "Total T: 27174 Episode Num: 174 Reward: 8.682859 Avg Reward: -8.418927saving best model....\n",
      "\n",
      "Total T: 27425 Episode Num: 175 Reward: -1.736073 Avg Reward: -8.182144saving best model....\n",
      "\n",
      "Total T: 27522 Episode Num: 176 Reward: 10.821220 Avg Reward: -7.944496saving best model....\n",
      "\n",
      "Total T: 28008 Episode Num: 179 Reward: -20.497341 Avg Reward: -7.835393saving best model....\n",
      "\n",
      "Total T: 28066 Episode Num: 180 Reward: 9.866287 Avg Reward: -7.618119saving best model....\n",
      "\n",
      "Total T: 28116 Episode Num: 181 Reward: 10.945159 Avg Reward: -7.535671saving best model....\n",
      "\n",
      "Total T: 28251 Episode Num: 182 Reward: -14.332769 Avg Reward: -7.517839saving best model....\n",
      "\n",
      "Total T: 28744 Episode Num: 186 Reward: -12.140910 Avg Reward: -7.651214saving best model....\n",
      "\n",
      "Total T: 30015 Episode Num: 193 Reward: -10.581056 Avg Reward: -7.360430saving best model....\n",
      "\n",
      "Total T: 30341 Episode Num: 196 Reward: -12.212489 Avg Reward: -7.375037saving best model....\n",
      "\n",
      "Total T: 32018 Episode Num: 208 Reward: -12.293283 Avg Reward: -7.067224saving best model....\n",
      "\n",
      "Total T: 32263 Episode Num: 210 Reward: -20.490759 Avg Reward: -6.980836saving best model....\n",
      "\n",
      "Total T: 32514 Episode Num: 211 Reward: -13.454504 Avg Reward: -6.902113saving best model....\n",
      "\n",
      "Total T: 32558 Episode Num: 212 Reward: -13.044331 Avg Reward: -6.862586saving best model....\n",
      "\n",
      "Total T: 33783 Episode Num: 221 Reward: 2.894884 Avg Reward: -6.94047703saving best model....\n",
      "\n",
      "Total T: 66697 Episode Num: 432 Reward: -12.673107 Avg Reward: -6.8794246saving best model....\n",
      "\n",
      "Total T: 66948 Episode Num: 433 Reward: -2.380456 Avg Reward: -6.712981saving best model....\n",
      "\n",
      "Total T: 67199 Episode Num: 434 Reward: -1.382367 Avg Reward: -6.561788saving best model....\n",
      "\n",
      "Total T: 67242 Episode Num: 435 Reward: 11.171290 Avg Reward: -6.560312saving best model....\n",
      "\n",
      "Total T: 67493 Episode Num: 436 Reward: -2.662238 Avg Reward: -6.539838saving best model....\n",
      "\n",
      "Total T: 67744 Episode Num: 437 Reward: -2.418402 Avg Reward: -6.285214saving best model....\n",
      "\n",
      "Total T: 67995 Episode Num: 438 Reward: -8.726538 Avg Reward: -6.236223saving best model....\n",
      "\n",
      "Total T: 68465 Episode Num: 440 Reward: -3.387721 Avg Reward: -6.027858saving best model....\n",
      "\n",
      "Total T: 71342 Episode Num: 461 Reward: -13.979411 Avg Reward: -6.017135saving best model....\n",
      "\n",
      "Total T: 75157 Episode Num: 489 Reward: 11.280519 Avg Reward: -5.9033860saving best model....\n",
      "\n",
      "Total T: 75247 Episode Num: 490 Reward: 8.505494 Avg Reward: -5.690534saving best model....\n",
      "\n",
      "Total T: 76286 Episode Num: 498 Reward: -2.754640 Avg Reward: -5.7738453saving best model....\n",
      "\n",
      "Total T: 76328 Episode Num: 499 Reward: 11.211175 Avg Reward: -5.503662saving best model....\n",
      "\n",
      "Total T: 76579 Episode Num: 500 Reward: -2.155366 Avg Reward: -5.484620saving best model....\n",
      "\n",
      "Total T: 76789 Episode Num: 501 Reward: -2.336664 Avg Reward: -5.364340saving best model....\n",
      "\n",
      "Total T: 92207 Episode Num: 612 Reward: -3.212676 Avg Reward: -5.3965876saving best model....\n",
      "\n",
      "Total T: 92311 Episode Num: 613 Reward: 8.878667 Avg Reward: -5.161080saving best model....\n",
      "\n",
      "Total T: 92562 Episode Num: 614 Reward: -2.288642 Avg Reward: -5.054323saving best model....\n",
      "\n",
      "Total T: 92917 Episode Num: 617 Reward: 10.613760 Avg Reward: -5.0181853saving best model....\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 93168 Episode Num: 618 Reward: -6.933859 Avg Reward: -4.887609saving best model....\n",
      "\n",
      "Total T: 93268 Episode Num: 619 Reward: 9.411656 Avg Reward: -4.644732saving best model....\n",
      "\n",
      "Total T: 93339 Episode Num: 620 Reward: 9.997490 Avg Reward: -4.423510saving best model....\n",
      "\n",
      "Total T: 95199 Episode Num: 630 Reward: -0.884660 Avg Reward: -4.3184994saving best model....\n",
      "\n",
      "Total T: 95247 Episode Num: 631 Reward: 11.216182 Avg Reward: -4.070476saving best model....\n",
      "\n",
      "Total T: 95311 Episode Num: 632 Reward: 9.563922 Avg Reward: -3.827933saving best model....\n",
      "\n",
      "Total T: 95396 Episode Num: 634 Reward: -13.049330 Avg Reward: -4.027586saving best model....\n",
      "\n",
      "Total T: 98052 Episode Num: 649 Reward: -12.524358 Avg Reward: -3.950044saving best model....\n",
      "\n",
      "Total T: 98183 Episode Num: 650 Reward: 13.312094 Avg Reward: -3.696635saving best model....\n",
      "\n",
      "Total T: 98232 Episode Num: 651 Reward: 10.913971 Avg Reward: -3.542972saving best model....\n",
      "\n",
      "Total T: 102082 Episode Num: 675 Reward: 11.310866 Avg Reward: -3.5335948saving best model....\n",
      "\n",
      "Total T: 119677 Episode Num: 792 Reward: -2.049137 Avg Reward: -3.4551907saving best model....\n",
      "\n",
      "Total T: 120082 Episode Num: 794 Reward: -14.551008 Avg Reward: -3.516087saving best model....\n",
      "\n",
      "Total T: 121551 Episode Num: 806 Reward: -8.620319 Avg Reward: -3.3194411saving best model....\n",
      "\n",
      "Total T: 129385 Episode Num: 854 Reward: 9.089563 Avg Reward: -3.28161719saving best model....\n",
      "\n",
      "Total T: 130004 Episode Num: 860 Reward: -1.840116 Avg Reward: -3.3512599saving best model....\n",
      "\n",
      "Total T: 130251 Episode Num: 863 Reward: -14.317084 Avg Reward: -3.132443saving best model....\n",
      "\n",
      "Total T: 130549 Episode Num: 865 Reward: 10.960202 Avg Reward: -3.053382saving best model....\n",
      "\n",
      "Total T: 130597 Episode Num: 866 Reward: -13.366404 Avg Reward: -3.005413saving best model....\n",
      "\n",
      "Total T: 131139 Episode Num: 869 Reward: -3.101744 Avg Reward: -3.1460482saving best model....\n",
      "\n",
      "Total T: 131248 Episode Num: 870 Reward: 9.412672 Avg Reward: -2.862707saving best model....\n",
      "\n",
      "Total T: 131303 Episode Num: 871 Reward: 10.099739 Avg Reward: -2.630755saving best model....\n",
      "\n",
      "Total T: 131373 Episode Num: 872 Reward: 9.110329 Avg Reward: -2.496359saving best model....\n",
      "\n",
      "Total T: 131796 Episode Num: 874 Reward: -15.544412 Avg Reward: -2.387240saving best model....\n",
      "\n",
      "Total T: 131856 Episode Num: 875 Reward: 8.911217 Avg Reward: -2.121339saving best model....\n",
      "\n",
      "Total T: 131997 Episode Num: 877 Reward: 8.268667 Avg Reward: -1.9254911saving best model....\n",
      "\n",
      "Total T: 137435 Episode Num: 913 Reward: -14.202539 Avg Reward: -1.964765saving best model....\n",
      "\n",
      "Total T: 137782 Episode Num: 915 Reward: -2.786134 Avg Reward: -1.892852saving best model....\n",
      "\n",
      "Total T: 140722 Episode Num: 933 Reward: 9.538814 Avg Reward: -1.80081626saving best model....\n",
      "\n",
      "Total T: 140904 Episode Num: 934 Reward: 8.354997 Avg Reward: -1.594315saving best model....\n",
      "\n",
      "Total T: 140941 Episode Num: 935 Reward: 11.659292 Avg Reward: -1.583039saving best model....\n",
      "\n",
      "Total T: 148970 Episode Num: 989 Reward: 9.189873 Avg Reward: -1.76520204saving best model....\n",
      "\n",
      "Total T: 149030 Episode Num: 990 Reward: 9.937922 Avg Reward: -1.483486saving best model....\n",
      "\n",
      "Total T: 149078 Episode Num: 991 Reward: 10.839337 Avg Reward: -1.122348saving best model....\n",
      "\n",
      "Total T: 149203 Episode Num: 992 Reward: 10.855921 Avg Reward: -0.888400saving best model....\n",
      "\n",
      "Total T: 149236 Episode Num: 993 Reward: 11.335861 Avg Reward: -0.736580saving best model....\n",
      "\n",
      "Total T: 149444 Episode Num: 994 Reward: 11.192169 Avg Reward: -0.702871saving best model....\n",
      "\n",
      "Total T: 152195 Episode Num: 1016 Reward: -0.940160 Avg Reward: -0.5877004saving best model....\n",
      "\n",
      "Total T: 152243 Episode Num: 1017 Reward: 10.566569 Avg Reward: -0.337096saving best model....\n",
      "\n",
      "Total T: 152494 Episode Num: 1018 Reward: -2.986375 Avg Reward: -0.330945saving best model....\n",
      "\n",
      "Total T: 152539 Episode Num: 1019 Reward: 11.313232 Avg Reward: -0.319215saving best model....\n",
      "\n",
      "Total T: 152753 Episode Num: 1020 Reward: 8.551338 Avg Reward: -0.200607saving best model....\n",
      "\n",
      "Total T: 152800 Episode Num: 1021 Reward: 11.236644 Avg Reward: -0.185127saving best model....\n",
      "\n",
      "Total T: 153553 Episode Num: 1024 Reward: -6.989872 Avg Reward: -0.142085saving best model....\n",
      "\n",
      "Total T: 153804 Episode Num: 1025 Reward: -1.872557 Avg Reward: 0.028326saving best model....\n",
      "\n",
      "Total T: 154569 Episode Num: 1030 Reward: -1.036345 Avg Reward: -0.0129408saving best model....\n",
      "\n",
      "Total T: 154608 Episode Num: 1031 Reward: 11.625470 Avg Reward: 0.274457saving best model....\n",
      "\n",
      "Total T: 154754 Episode Num: 1032 Reward: 13.071264 Avg Reward: 0.476737saving best model....\n",
      "\n",
      "Total T: 155752 Episode Num: 1040 Reward: -14.307390 Avg Reward: 0.414847saving best model....\n",
      "\n",
      "Total T: 155789 Episode Num: 1041 Reward: 11.634538 Avg Reward: 0.587380saving best model....\n",
      "\n",
      "Total T: 156040 Episode Num: 1042 Reward: -6.244324 Avg Reward: 0.592593saving best model....\n",
      "\n",
      "Total T: 156725 Episode Num: 1047 Reward: 9.699899 Avg Reward: 0.44862228saving best model....\n",
      "\n",
      "Total T: 156798 Episode Num: 1048 Reward: 10.910431 Avg Reward: 0.809014saving best model....\n",
      "\n",
      "Total T: 156863 Episode Num: 1049 Reward: 9.241848 Avg Reward: 0.962996saving best model....\n",
      "\n",
      "Total T: 157632 Episode Num: 1054 Reward: 10.927095 Avg Reward: 1.0322793saving best model....\n",
      "\n",
      "Total T: 159078 Episode Num: 1066 Reward: 11.812917 Avg Reward: 1.1246297saving best model....\n",
      "\n",
      "Total T: 159132 Episode Num: 1067 Reward: 10.562779 Avg Reward: 1.283843saving best model....\n",
      "\n",
      "Total T: 209788 Episode Num: 1447 Reward: 8.659750 Avg Reward: 1.193836764saving best model....\n",
      "\n",
      "Total T: 210037 Episode Num: 1451 Reward: 11.536114 Avg Reward: 1.2595608saving best model....\n",
      "\n",
      "Total T: 211176 Episode Num: 1463 Reward: 9.953899 Avg Reward: 1.36134135saving best model....\n",
      "\n",
      "Total T: 211932 Episode Num: 1467 Reward: -2.276725 Avg Reward: 1.493341saving best model....\n",
      "\n",
      "Total T: 214976 Episode Num: 1491 Reward: 12.414719 Avg Reward: 1.5877194saving best model....\n",
      "\n",
      "Total T: 245845 Episode Num: 1713 Reward: 9.594701 Avg Reward: 1.747748388saving best model....\n",
      "\n",
      "Total T: 246992 Episode Num: 1723 Reward: 8.698263 Avg Reward: 1.80675092saving best model....\n",
      "\n",
      "Total T: 247243 Episode Num: 1724 Reward: -2.329587 Avg Reward: 1.900583saving best model....\n",
      "\n",
      "Total T: 247381 Episode Num: 1725 Reward: 10.982135 Avg Reward: 2.020090saving best model....\n",
      "\n",
      "Total T: 247632 Episode Num: 1726 Reward: -0.025933 Avg Reward: 2.153761saving best model....\n",
      "\n",
      "Total T: 247671 Episode Num: 1727 Reward: 10.754399 Avg Reward: 2.160499saving best model....\n",
      "\n",
      "Total T: 247772 Episode Num: 1728 Reward: 12.088837 Avg Reward: 2.189834saving best model....\n",
      "\n",
      "Total T: 247812 Episode Num: 1729 Reward: 10.776231 Avg Reward: 2.201472saving best model....\n",
      "\n",
      "Total T: 249421 Episode Num: 1737 Reward: -16.268864 Avg Reward: 2.294944saving best model....\n",
      "\n",
      "Total T: 249526 Episode Num: 1739 Reward: 9.877612 Avg Reward: 2.4350095saving best model....\n",
      "\n",
      "Total T: 250028 Episode Num: 1741 Reward: -1.353383 Avg Reward: 2.441152saving best model....\n",
      "\n",
      "Total T: 250993 Episode Num: 1747 Reward: 10.854318 Avg Reward: 2.549548saving best model....\n",
      "\n",
      "Total T: 262492 Episode Num: 1836 Reward: -6.722741 Avg Reward: 2.3490117saving best model....\n",
      "\n",
      "Total T: 263046 Episode Num: 1842 Reward: 9.874971 Avg Reward: 2.47411750saving best model....\n",
      "\n",
      "Total T: 263904 Episode Num: 1850 Reward: -0.425557 Avg Reward: 2.5808366saving best model....\n",
      "\n",
      "Total T: 264264 Episode Num: 1853 Reward: 10.178874 Avg Reward: 2.709988saving best model....\n",
      "\n",
      "Total T: 264677 Episode Num: 1856 Reward: -11.676206 Avg Reward: 2.766653saving best model....\n",
      "\n",
      "Total T: 265316 Episode Num: 1861 Reward: 10.319666 Avg Reward: 2.904460saving best model....\n",
      "\n",
      "Total T: 318447 Episode Num: 2251 Reward: 9.451521 Avg Reward: 2.98439051saving best model....\n",
      "\n",
      "Total T: 336392 Episode Num: 2372 Reward: -0.148036 Avg Reward: 2.9896704saving best model....\n",
      "\n",
      "Total T: 336643 Episode Num: 2373 Reward: -2.912658 Avg Reward: 3.099404saving best model....\n",
      "\n",
      "Total T: 337478 Episode Num: 2378 Reward: -1.338923 Avg Reward: 3.2000704saving best model....\n",
      "\n",
      "Total T: 337785 Episode Num: 2380 Reward: -3.777627 Avg Reward: 3.296190saving best model....\n",
      "\n",
      "Total T: 344010 Episode Num: 2425 Reward: 11.675706 Avg Reward: 3.5040852saving best model....\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 344261 Episode Num: 2426 Reward: 1.265070 Avg Reward: 3.643268saving best model....\n",
      "\n",
      "Total T: 345446 Episode Num: 2436 Reward: 9.740023 Avg Reward: 3.66192812saving best model....\n",
      "\n",
      "Total T: 345497 Episode Num: 2437 Reward: 9.707677 Avg Reward: 3.758979saving best model....\n",
      "\n",
      "Total T: 345584 Episode Num: 2438 Reward: 9.378686 Avg Reward: 3.894240saving best model....\n",
      "\n",
      "Total T: 360379 Episode Num: 2548 Reward: 12.588794 Avg Reward: 4.1014308saving best model....\n",
      "\n",
      "Total T: 360416 Episode Num: 2549 Reward: 11.517106 Avg Reward: 4.235068saving best model....\n",
      "\n",
      "Total T: 360486 Episode Num: 2550 Reward: 10.865837 Avg Reward: 4.466325saving best model....\n",
      "\n",
      "Total T: 360777 Episode Num: 2552 Reward: -2.161423 Avg Reward: 4.368262saving best model....\n",
      "\n",
      "Total T: 361086 Episode Num: 2554 Reward: -4.323249 Avg Reward: 4.466999saving best model....\n",
      "\n",
      "Total T: 361153 Episode Num: 2555 Reward: 9.468358 Avg Reward: 4.557774saving best model....\n",
      "\n",
      "Total T: 361403 Episode Num: 2556 Reward: 8.067551 Avg Reward: 4.782607saving best model....\n",
      "\n",
      "Total T: 361475 Episode Num: 2557 Reward: 9.144991 Avg Reward: 4.788848saving best model....\n",
      "\n",
      "Total T: 395260 Episode Num: 2799 Reward: 10.025269 Avg Reward: 4.6983220saving best model....\n",
      "\n",
      "Total T: 396038 Episode Num: 2804 Reward: 11.977867 Avg Reward: 4.683366saving best model....\n",
      "\n",
      "Total T: 396116 Episode Num: 2805 Reward: 7.677049 Avg Reward: 4.885054saving best model....\n",
      "\n",
      "Total T: 398536 Episode Num: 2826 Reward: -1.275049 Avg Reward: 4.7964486saving best model....\n",
      "\n",
      "Total T: 400642 Episode Num: 2846 Reward: -3.744909 Avg Reward: 4.8954887saving best model....\n",
      "\n",
      "Total T: 400888 Episode Num: 2849 Reward: 11.200107 Avg Reward: 5.0379962saving best model....\n",
      "\n",
      "Total T: 400976 Episode Num: 2850 Reward: 9.291460 Avg Reward: 5.144985saving best model....\n",
      "\n",
      "Total T: 444606 Episode Num: 3170 Reward: -1.417468 Avg Reward: 5.1029001saving best model....\n",
      "\n",
      "Total T: 446482 Episode Num: 3181 Reward: 10.110261 Avg Reward: 5.119558saving best model....\n",
      "\n",
      "Total T: 447015 Episode Num: 3184 Reward: -1.931473 Avg Reward: 5.190704saving best model....\n",
      "\n",
      "Total T: 447754 Episode Num: 3190 Reward: -0.124663 Avg Reward: 5.194565saving best model....\n",
      "\n",
      "Total T: 449423 Episode Num: 3201 Reward: 11.239724 Avg Reward: 5.4046774saving best model....\n",
      "\n",
      "Total T: 449674 Episode Num: 3202 Reward: -0.404730 Avg Reward: 5.531367saving best model....\n",
      "\n",
      "Total T: 449727 Episode Num: 3203 Reward: 10.134232 Avg Reward: 5.533065saving best model....\n",
      "\n",
      "Total T: 482466 Episode Num: 3429 Reward: 10.334014 Avg Reward: 5.5204626saving best model....\n",
      "\n",
      "Total T: 482717 Episode Num: 3430 Reward: -0.384104 Avg Reward: 5.561005saving best model....\n",
      "\n",
      "Total T: 484251 Episode Num: 3442 Reward: 1.658986 Avg Reward: 5.37800336saving best model....\n",
      "\n",
      "Total T: 503007 Episode Num: 3585 Reward: 10.868109 Avg Reward: 5.6034058saving best model....\n",
      "\n",
      "Total T: 503133 Episode Num: 3586 Reward: 10.819962 Avg Reward: 5.697947saving best model....\n",
      "\n",
      "Total T: 503184 Episode Num: 3587 Reward: 10.187388 Avg Reward: 5.843200saving best model....\n",
      "\n",
      "Total T: 538726 Episode Num: 3840 Reward: -13.741239 Avg Reward: 4.971551"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-86d5ed22ecc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-138-dc1138cf455d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agent, test_env)\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTAU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNOISE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNOISE_CLIP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPOLICY_FREQUENCY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;31m# Evaluate episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-134-4168e30ad0d2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, replay_buffer, iterations, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m# Get current Q estimates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mcurrent_Q1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_Q2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Compute critic loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-132-49c88becdb53>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, u)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train agent\n",
    "train(policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------\n",
      "Evaluation over 100 episodes: 4.900000\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.9"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.load(filename=\"best_avg\", directory=\"TD3/saves\")\n",
    "\n",
    "evaluate_policy(policy, env, eval_episodes=100, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twin Delayed Deep Deterministic Policy Gradient (TD3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "from laserhockey.hockey_env import BasicOpponent\n",
    "from laserhockey.TrainingHall import TrainingHall\n",
    "import gym\n",
    "#import roboschool\n",
    "import sys\n",
    "from DDPG.ddpg_agent import DDPGAgent\n",
    "from DQN.agent import DQNAgent\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Initialize parameters and build model.\n",
    "        Args:\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            max_action (float): highest action to take\n",
    "            seed (int): Random seed\n",
    "            h1_units (int): Number of nodes in first hidden layer\n",
    "            h2_units (int): Number of nodes in second hidden layer\n",
    "            \n",
    "        Return:\n",
    "            action output of network with tanh activation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, action_dim)\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.max_action * torch.tanh(self.l3(x)) \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Initialize parameters and build model.\n",
    "        Args:\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            max_action (float): highest action to take\n",
    "            seed (int): Random seed\n",
    "            h1_units (int): Number of nodes in first hidden layer\n",
    "            h2_units (int): Number of nodes in second hidden layer\n",
    "            \n",
    "        Return:\n",
    "            value output of network \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.l4 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.l5 = nn.Linear(400, 300)\n",
    "        self.l6 = nn.Linear(300, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "\n",
    "        x1 = F.relu(self.l1(xu))\n",
    "        x1 = F.relu(self.l2(x1))\n",
    "        x1 = self.l3(x1)\n",
    "\n",
    "        x2 = F.relu(self.l4(xu))\n",
    "        x2 = F.relu(self.l5(x2))\n",
    "        x2 = self.l6(x2)\n",
    "        return x1, x2\n",
    "\n",
    "\n",
    "    def Q1(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "\n",
    "        x1 = F.relu(self.l1(xu))\n",
    "        x1 = F.relu(self.l2(x1))\n",
    "        x1 = self.l3(x1)\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code based on: \n",
    "# https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
    "\n",
    "# Expects tuples of (state, next_state, action, reward, done)\n",
    "class ReplayBuffer(object):\n",
    "    \"\"\"Buffer to store tuples of experience replay\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size=150000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_size (int): total amount of tuples to store\n",
    "        \"\"\"\n",
    "        \n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    def add(self, data):\n",
    "        \"\"\"Add experience tuples to buffer\n",
    "        \n",
    "        Args:\n",
    "            data (tuple): experience replay tuple\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = data\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Samples a random amount of experiences from buffer of batch size\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): size of sample\n",
    "        \"\"\"\n",
    "        \n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        states, actions, next_states, rewards, dones = [], [], [], [], []\n",
    "\n",
    "        for i in ind: \n",
    "            s, a, s_, r, d = self.storage[i]\n",
    "            states.append(np.array(s, copy=False))\n",
    "            actions.append(np.array(a, copy=False))\n",
    "            next_states.append(np.array(s_, copy=False))\n",
    "            rewards.append(np.array(r, copy=False))\n",
    "            dones.append(np.array(d, copy=False))\n",
    "\n",
    "        return np.array(states), np.array(actions), np.array(next_states), np.array(rewards).reshape(-1, 1), np.array(dones).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(object):\n",
    "    \"\"\"Agent class that handles the training of the networks and provides outputs as actions\n",
    "    \n",
    "        Args:\n",
    "            state_dim (int): state size\n",
    "            action_dim (int): action size\n",
    "            max_action (float): highest action to take\n",
    "            device (device): cuda or cpu to process tensors\n",
    "            env (env): gym environment to use\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, observation_space, action_space, pretrained=False):\n",
    "        state_dim  = observation_space.shape[0]\n",
    "        action_dim = action_space.shape[0]\n",
    "        max_action = float(action_space.high[0])\n",
    "        \n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-3)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        #self.scaling = torch.tensor([ 5.0,  5.0 , 5.0, 3.0, 3.0, 3.0,  \n",
    "        #                                2.0,  2.0,  1.0, 1.0, 1.0, 1.0,  \n",
    "        #                               10.0, 10.0, 10.0, 10.0, 5.0 ,5.0])\n",
    "        if pretrained:\n",
    "            self.load(pretrained)\n",
    "\n",
    "\n",
    "        \n",
    "    def act(self, state, eps=0.0):\n",
    "        \"\"\"Select an appropriate action from the agent policy\n",
    "        \n",
    "            Args:\n",
    "                state (array): current state of environment\n",
    "                eps (float): how much noise to add to acitons\n",
    "                \n",
    "            Returns:\n",
    "                action (float): action clipped within action range\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)#*self.scaling\n",
    "        action = self.actor(state).cpu().data.numpy().flatten()\n",
    "        if eps != 0: \n",
    "            action = (action + np.random.normal(0, eps, size=self.action_space.shape[0]))\n",
    "            \n",
    "        return action.clip(self.action_space.low, self.action_space.high)\n",
    "\n",
    "    \n",
    "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "        \"\"\"Train and update actor and critic networks\n",
    "        \n",
    "            Args:\n",
    "                replay_buffer (ReplayBuffer): buffer for experience replay\n",
    "                iterations (int): how many times to run training\n",
    "                batch_size(int): batch size to sample from replay buffer\n",
    "                discount (float): discount factor\n",
    "                tau (float): soft update for main networks to target networks\n",
    "                \n",
    "            Return:\n",
    "                actor_loss (float): loss from actor network\n",
    "                critic_loss (float): loss from critic network\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        for it in range(iterations):\n",
    "\n",
    "            # Sample replay buffer \n",
    "            x, y, u, r, d = replay_buffer.sample(batch_size)\n",
    "            state = torch.FloatTensor(x).to(device)#*self.scaling\n",
    "            action = torch.FloatTensor(u).to(device)\n",
    "            next_state = torch.FloatTensor(y).to(device)#*self.scaling\n",
    "            done = torch.FloatTensor(1 - d).to(device)\n",
    "            reward = torch.FloatTensor(r).to(device)\n",
    "            \n",
    "            # #add noise to observation\n",
    "            #clean_idx = (torch.tensor([ 0,1,2,3,4,5,12,13,14,15, 16, 17]),)\n",
    "            #noise = torch.FloatTensor(state).data.normal_(0, 1).to(device)\n",
    "            #noise[clean_idx] = 0\n",
    "            #state += noise\n",
    "\n",
    "            # Select action according to policy and add clipped noise \n",
    "            noise = torch.FloatTensor(u).data.normal_(0, policy_noise).to(device)\n",
    "            noise = noise.clamp(-noise_clip, noise_clip)\n",
    "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            # Compute the target Q value\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + (done * discount * target_Q).detach()\n",
    "\n",
    "            # Get current Q estimates\n",
    "            current_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "            # Compute critic loss\n",
    "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q) \n",
    "\n",
    "            # Optimize the critic\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            # Delayed policy updates\n",
    "            if it % policy_freq == 0:\n",
    "\n",
    "                # Compute actor loss\n",
    "                actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "\n",
    "                # Optimize the actor \n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                # Update the frozen target models\n",
    "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "\n",
    "    def save(self, filename, directory):\n",
    "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "\n",
    "\n",
    "    def load(self, filename=\"best_avg\", directory=\"TD3/saves\"):\n",
    "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner():\n",
    "    \"\"\"Carries out the environment steps and adds experiences to memory\"\"\"\n",
    "    \n",
    "    def __init__(self, env, agent, replay_buffer):\n",
    "        \n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.obs = env.reset()\n",
    "        self.done = False\n",
    "        \n",
    "    def next_step(self, episode_timesteps, noise=0.1):\n",
    "        \n",
    "        action = self.agent.act(np.array(self.obs), eps=0.1)\n",
    "\n",
    "        # Perform action\n",
    "        new_obs, reward, done, info = self.env.step(action) \n",
    "        done_bool = 0 if episode_timesteps + 1 == 255 else float(done)\n",
    "        \n",
    "        #add proxy\n",
    "        reward += sum(list(info.values()))\n",
    "        \n",
    "        # Store data in replay buffer\n",
    "        replay_buffer.add((self.obs, new_obs, action, reward, done_bool))\n",
    "        \n",
    "        self.obs = new_obs\n",
    "        \n",
    "        if done:\n",
    "            self.obs = self.env.reset()\n",
    "            done = False\n",
    "            \n",
    "            return reward, True\n",
    "        \n",
    "        return reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, env, eval_episodes=100,render=False):\n",
    "    \"\"\"run several episodes using the best agent policy\n",
    "        \n",
    "        Args:\n",
    "            policy (agent): agent to evaluate\n",
    "            env (env): gym environment\n",
    "            eval_episodes (int): how many test episodes to run\n",
    "            render (bool): show training\n",
    "        \n",
    "        Returns:\n",
    "            avg_reward (float): average reward over the number of evaluations\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    avg_reward = 0.\n",
    "    for i in range(eval_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = policy.act(np.array(obs), eps=0)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "\n",
    "    print(\"\\n---------------------------------------\")\n",
    "    print(\"Evaluation over {:d} episodes: {:f}\" .format(eval_episodes, avg_reward))\n",
    "    print(\"---------------------------------------\")\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe(env,replay_buffer, observation_steps):\n",
    "    \"\"\"run episodes while taking random actions and filling replay_buffer\n",
    "    \n",
    "        Args:\n",
    "            env (env): gym environment\n",
    "            replay_buffer(ReplayBuffer): buffer to store experience replay\n",
    "            observation_steps (int): how many steps to observe for\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    time_steps = 0\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while time_steps < observation_steps:\n",
    "        action = env.action_space.sample()\n",
    "        new_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        replay_buffer.add((obs, new_obs, action, reward, done))\n",
    "\n",
    "        obs = new_obs\n",
    "        time_steps += 1\n",
    "\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "\n",
    "        print(\"\\rPopulating Buffer {}/{}.\".format(time_steps, observation_steps), end=\"\")\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env):\n",
    "    \"\"\"Train the agent for exploration steps\n",
    "    \n",
    "        Args:\n",
    "            agent (Agent): agent to use\n",
    "            env (environment): gym environment\n",
    "            writer (SummaryWriter): tensorboard writer\n",
    "            exploration (int): how many training steps to run\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    total_timesteps = 0\n",
    "    timesteps_since_eval = 0\n",
    "    episode_num = 0\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    done = False \n",
    "    evaluations = []\n",
    "    rewards = []\n",
    "    best_avg = -5\n",
    "    \n",
    "    writer = SummaryWriter(comment=\"-last\")\n",
    "    \n",
    "    while total_timesteps < EXPLORATION:\n",
    "    \n",
    "        if done and total_timesteps != 0:\n",
    "\n",
    "            rewards.append(episode_reward)\n",
    "            avg_reward = np.mean(rewards[-100:])\n",
    "\n",
    "            writer.add_scalar(\"avg_reward\", avg_reward, total_timesteps)\n",
    "            writer.add_scalar(\"reward_step\", reward, total_timesteps)\n",
    "            writer.add_scalar(\"episode_reward\", episode_reward, total_timesteps)\n",
    "\n",
    "            if best_avg < avg_reward:\n",
    "                best_avg = avg_reward\n",
    "                print(\"saving best model....\\n\")\n",
    "                agent.save(\"best_avg\",\"TD3/saves\")\n",
    "\n",
    "            print(\"\\rTotal T: {:d} Episode Num: {:d} Reward: {:f} Avg Reward: {:f}\".format(\n",
    "                total_timesteps, episode_num, episode_reward, avg_reward), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "\n",
    "            if avg_reward >= REWARD_THRESH:\n",
    "                break\n",
    "\n",
    "            agent.train(replay_buffer, \n",
    "                        episode_timesteps, \n",
    "                        BATCH_SIZE, GAMMA, TAU, NOISE, NOISE_CLIP, POLICY_FREQUENCY)\n",
    "\n",
    "            episode_reward = 0\n",
    "            episode_timesteps = 0\n",
    "            episode_num += 1 \n",
    "\n",
    "        reward, done = runner.next_step(episode_timesteps)\n",
    "        episode_reward += reward\n",
    "\n",
    "        episode_timesteps += 1\n",
    "        total_timesteps += 1\n",
    "        timesteps_since_eval += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SEED = 5\n",
    "OBSERVATION = 10000\n",
    "EXPLORATION = 1000000\n",
    "BATCH_SIZE = 100\n",
    "GAMMA = 0.99\n",
    "TAU = 0.005\n",
    "NOISE = 0.2\n",
    "NOISE_CLIP = 0.5\n",
    "EXPLORE_NOISE = 0.2\n",
    "POLICY_FREQUENCY = 2\n",
    "EVAL_FREQUENCY = 5000\n",
    "REWARD_THRESH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "current winratios: [0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = TrainingHall()\n",
    "\n",
    "basic  = BasicOpponent(weak=False)\n",
    "td3   = TD3(env.observation_space, env.action_space, pretrained='stronger')\n",
    "loser = TD3(env.observation_space, env.action_space, pretrained='smallhall')\n",
    "ddpg  = DDPGAgent(env,\n",
    "                actor_lr=1e-4,\n",
    "                critic_lr=1e-3,\n",
    "                update_rate=0.05,\n",
    "                discount=0.9, update_target_every=20,\n",
    "                pretrained='DDPG/weights/ddpg-normal-eps-noise-basic-35000')\n",
    "q_agent = DQNAgent(env.observation_space, \n",
    "                         env.discrete_action_space,\n",
    "                        convert_func =  env.discrete_to_continous_action,\n",
    "                        pretrained   = 'DQN/weights/training_hall_1')\n",
    "\n",
    "env.register_opponents([basic])#,ddpg,q_agent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "current winratios: [0. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set seeds\n",
    "env.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "policy = TD3(env.observation_space, env.action_space, pretrained=False)\n",
    "\n",
    "replay_buffer = ReplayBuffer(max_size=150000)\n",
    "\n",
    "runner = Runner(env, policy, replay_buffer)\n",
    "\n",
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating Buffer 10000/10000."
     ]
    }
   ],
   "source": [
    "# Populate replay buffer\n",
    "observe(env, replay_buffer, OBSERVATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0.])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<laserhockey.hockey_env.BasicOpponent object at 0x7f4d838a1e80>,\n",
       "       <laserhockey.hockey_env.BasicOpponent object at 0x7f4d838a1dc0>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 6224 Episode Num: 36 Reward: -12.646000 Avg Reward: -6.865821\n",
      "current winratios: [-0.30612245 -0.21818182]\n",
      "\n",
      "Total T: 21648 Episode Num: 136 Reward: -15.803557 Avg Reward: -10.239544\n",
      "current winratios: [-0.41 -0.23]\n",
      "\n",
      "Total T: 40987 Episode Num: 236 Reward: 15.513717 Avg Reward: -7.29871242\n",
      "current winratios: [-0.36 -0.24]\n",
      "\n",
      "Total T: 58730 Episode Num: 336 Reward: -14.140107 Avg Reward: -6.055348\n",
      "current winratios: [-0.27 -0.2 ]\n",
      "\n",
      "Total T: 76410 Episode Num: 436 Reward: -2.505574 Avg Reward: -5.8041385\n",
      "current winratios: [-0.28 -0.22]\n",
      "\n",
      "Total T: 92075 Episode Num: 518 Reward: -14.190922 Avg Reward: -5.187812saving best model....\n",
      "\n",
      "Total T: 95358 Episode Num: 536 Reward: -9.761144 Avg Reward: -5.5712249\n",
      "current winratios: [-0.33 -0.19]\n",
      "\n",
      "Total T: 114739 Episode Num: 636 Reward: -16.352077 Avg Reward: -6.769814\n",
      "current winratios: [-0.28 -0.21]\n",
      "\n",
      "Total T: 126542 Episode Num: 695 Reward: -5.843081 Avg Reward: -5.0166815saving best model....\n",
      "\n",
      "Total T: 126622 Episode Num: 696 Reward: 11.624206 Avg Reward: -4.746779saving best model....\n",
      "\n",
      "Total T: 126873 Episode Num: 697 Reward: -6.406522 Avg Reward: -4.738459saving best model....\n",
      "\n",
      "Total T: 128104 Episode Num: 704 Reward: -2.225109 Avg Reward: -4.5259570saving best model....\n",
      "\n",
      "Total T: 128355 Episode Num: 705 Reward: -5.795645 Avg Reward: -4.451650saving best model....\n",
      "\n",
      "Total T: 128606 Episode Num: 706 Reward: -7.424998 Avg Reward: -4.430811saving best model....\n",
      "\n",
      "Total T: 128857 Episode Num: 707 Reward: -4.041343 Avg Reward: -4.418940saving best model....\n",
      "\n",
      "Total T: 129108 Episode Num: 708 Reward: 6.809964 Avg Reward: -4.223614saving best model....\n",
      "\n",
      "Total T: 129359 Episode Num: 709 Reward: -5.627613 Avg Reward: -4.205737saving best model....\n",
      "\n",
      "Total T: 130077 Episode Num: 713 Reward: -6.146453 Avg Reward: -4.0588915saving best model....\n",
      "\n",
      "Total T: 130178 Episode Num: 714 Reward: 15.678344 Avg Reward: -3.894041saving best model....\n",
      "\n",
      "Total T: 130429 Episode Num: 715 Reward: -5.926368 Avg Reward: -3.866758saving best model....\n",
      "\n",
      "Total T: 130463 Episode Num: 716 Reward: 13.972677 Avg Reward: -3.780697saving best model....\n",
      "\n",
      "Total T: 131092 Episode Num: 719 Reward: -6.252501 Avg Reward: -3.766789saving best model....\n",
      "\n",
      "Total T: 134608 Episode Num: 736 Reward: 1.928596 Avg Reward: -3.69659418\n",
      "current winratios: [-0.16 -0.16]\n",
      "\n",
      "saving best model....\n",
      "\n",
      "Total T: 142629 Episode Num: 777 Reward: -5.536959 Avg Reward: -3.6485595saving best model....\n",
      "\n",
      "Total T: 142685 Episode Num: 778 Reward: 13.946727 Avg Reward: -3.532912saving best model....\n",
      "\n",
      "Total T: 142936 Episode Num: 779 Reward: -3.999591 Avg Reward: -3.510882saving best model....\n",
      "\n",
      "Total T: 143187 Episode Num: 780 Reward: 2.547255 Avg Reward: -3.367222saving best model....\n",
      "\n",
      "Total T: 154032 Episode Num: 836 Reward: -25.193239 Avg Reward: -4.970433\n",
      "current winratios: [-0.18 -0.17]\n",
      "\n",
      "Total T: 174279 Episode Num: 936 Reward: -13.808969 Avg Reward: -3.515129\n",
      "current winratios: [-0.16 -0.19]\n",
      "\n",
      "Total T: 175567 Episode Num: 943 Reward: -6.153714 Avg Reward: -3.4063470saving best model....\n",
      "\n",
      "Total T: 175656 Episode Num: 944 Reward: 10.968564 Avg Reward: -3.285621saving best model....\n",
      "\n",
      "Total T: 175907 Episode Num: 945 Reward: -5.429583 Avg Reward: -3.279170saving best model....\n",
      "\n",
      "Total T: 194079 Episode Num: 1036 Reward: 16.439535 Avg Reward: -3.3259199\n",
      "current winratios: [-0.11 -0.14]\n",
      "\n",
      "Total T: 194330 Episode Num: 1037 Reward: -4.526746 Avg Reward: -3.307831saving best model....\n",
      "\n",
      "Total T: 194832 Episode Num: 1039 Reward: -5.289708 Avg Reward: -3.190419saving best model....\n",
      "\n",
      "Total T: 195334 Episode Num: 1041 Reward: -4.976647 Avg Reward: -3.063036saving best model....\n",
      "\n",
      "Total T: 195399 Episode Num: 1042 Reward: 12.073102 Avg Reward: -2.816013saving best model....\n",
      "\n",
      "Total T: 195650 Episode Num: 1043 Reward: -4.791618 Avg Reward: -2.802392saving best model....\n",
      "\n",
      "Total T: 195730 Episode Num: 1044 Reward: 11.952868 Avg Reward: -2.792549saving best model....\n",
      "\n",
      "Total T: 197249 Episode Num: 1051 Reward: -3.291313 Avg Reward: -2.8743685saving best model....\n",
      "\n",
      "Total T: 197302 Episode Num: 1052 Reward: 14.930394 Avg Reward: -2.607006saving best model....\n",
      "\n",
      "Total T: 197553 Episode Num: 1053 Reward: -3.975044 Avg Reward: -2.605905saving best model....\n",
      "\n",
      "Total T: 200338 Episode Num: 1067 Reward: -5.415075 Avg Reward: -2.5106073saving best model....\n",
      "\n",
      "Total T: 200774 Episode Num: 1069 Reward: -3.915243 Avg Reward: -2.352370saving best model....\n",
      "\n",
      "Total T: 204899 Episode Num: 1089 Reward: -4.934319 Avg Reward: -2.6031938saving best model....\n",
      "\n",
      "Total T: 205031 Episode Num: 1090 Reward: 17.670089 Avg Reward: -2.287229saving best model....\n",
      "\n",
      "Total T: 205282 Episode Num: 1091 Reward: -4.575578 Avg Reward: -2.273049saving best model....\n",
      "\n",
      "Total T: 205997 Episode Num: 1095 Reward: -13.036078 Avg Reward: -2.123665saving best model....\n",
      "\n",
      "Total T: 206146 Episode Num: 1096 Reward: 12.461439 Avg Reward: -1.908261saving best model....\n",
      "\n",
      "Total T: 207142 Episode Num: 1101 Reward: -4.706700 Avg Reward: -1.9341176saving best model....\n",
      "\n",
      "Total T: 214874 Episode Num: 1136 Reward: 12.850132 Avg Reward: -2.3484909\n",
      "current winratios: [-0.2  -0.05]\n",
      "\n",
      "Total T: 235177 Episode Num: 1236 Reward: -14.906412 Avg Reward: -4.348450\n",
      "current winratios: [-0.24 -0.07]\n",
      "\n",
      "Total T: 255816 Episode Num: 1336 Reward: -1.137004 Avg Reward: -2.6754411\n",
      "current winratios: [-0.2  -0.13]\n",
      "\n",
      "Total T: 277042 Episode Num: 1436 Reward: -14.370673 Avg Reward: -2.774664\n",
      "current winratios: [-0.09 -0.13]\n",
      "\n",
      "Total T: 291126 Episode Num: 1501 Reward: -4.144674 Avg Reward: -1.7868113saving best model....\n",
      "\n",
      "Total T: 291161 Episode Num: 1502 Reward: 14.941990 Avg Reward: -1.609162saving best model....\n",
      "\n",
      "Total T: 291412 Episode Num: 1503 Reward: -5.005546 Avg Reward: -1.604148saving best model....\n",
      "\n",
      "Total T: 291463 Episode Num: 1504 Reward: 14.614352 Avg Reward: -1.486954saving best model....\n",
      "\n",
      "Total T: 293619 Episode Num: 1513 Reward: -4.555191 Avg Reward: -1.6189522saving best model....\n",
      "\n",
      "Total T: 294783 Episode Num: 1519 Reward: -3.946288 Avg Reward: -1.6331063saving best model....\n",
      "\n",
      "Total T: 294939 Episode Num: 1520 Reward: 12.776936 Avg Reward: -1.333138saving best model....\n",
      "\n",
      "Total T: 295190 Episode Num: 1521 Reward: -4.930191 Avg Reward: -1.332839saving best model....\n",
      "\n",
      "Total T: 295441 Episode Num: 1522 Reward: 8.851954 Avg Reward: -1.324321saving best model....\n",
      "\n",
      "Total T: 296455 Episode Num: 1527 Reward: -4.014156 Avg Reward: -1.429914saving best model....\n",
      "\n",
      "Total T: 298172 Episode Num: 1536 Reward: -10.827946 Avg Reward: -1.173140\n",
      "current winratios: [ 0.02 -0.17]\n",
      "\n",
      "Total T: 302120 Episode Num: 1553 Reward: 6.267519 Avg Reward: -1.1032248saving best model....\n",
      "\n",
      "Total T: 302556 Episode Num: 1555 Reward: -5.905196 Avg Reward: -0.840115saving best model....\n",
      "\n",
      "Total T: 302735 Episode Num: 1556 Reward: 9.706976 Avg Reward: -0.767698saving best model....\n",
      "\n",
      "Total T: 303488 Episode Num: 1559 Reward: -5.653448 Avg Reward: -0.826704saving best model....\n",
      "\n",
      "Total T: 303990 Episode Num: 1561 Reward: -5.748083 Avg Reward: -0.705429saving best model....\n",
      "\n",
      "Total T: 318771 Episode Num: 1636 Reward: -10.338388 Avg Reward: -2.770008\n",
      "current winratios: [ 0.02 -0.13]\n",
      "\n",
      "Total T: 340847 Episode Num: 1736 Reward: 4.492099 Avg Reward: -1.92695946\n",
      "current winratios: [-0.02 -0.14]\n",
      "\n",
      "Total T: 361498 Episode Num: 1836 Reward: -14.934858 Avg Reward: -2.184067\n",
      "current winratios: [-0.06 -0.07]\n",
      "\n",
      "Total T: 383524 Episode Num: 1936 Reward: 7.897409 Avg Reward: -2.25431704\n",
      "current winratios: [-0.09 -0.04]\n",
      "\n",
      "Total T: 396261 Episode Num: 1995 Reward: -5.318753 Avg Reward: -0.9227485saving best model....\n",
      "\n",
      "Total T: 396333 Episode Num: 1996 Reward: 14.410057 Avg Reward: -0.676362saving best model....\n",
      "\n",
      "Total T: 401350 Episode Num: 2019 Reward: -5.434710 Avg Reward: -0.8052583saving best model....\n",
      "\n",
      "Total T: 401740 Episode Num: 2021 Reward: -5.632802 Avg Reward: -0.600684saving best model....\n",
      "\n",
      "Total T: 402285 Episode Num: 2024 Reward: 5.906232 Avg Reward: -0.52864226saving best model....\n",
      "\n",
      "Total T: 403178 Episode Num: 2029 Reward: -4.839454 Avg Reward: -0.5565216saving best model....\n",
      "\n",
      "Total T: 403318 Episode Num: 2030 Reward: 13.657911 Avg Reward: -0.501870saving best model....\n",
      "\n",
      "Total T: 403569 Episode Num: 2031 Reward: -4.440697 Avg Reward: -0.492914saving best model....\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 404821 Episode Num: 2036 Reward: 4.920143 Avg Reward: -0.5410541\n",
      "current winratios: [-0.06  0.  ]\n",
      "\n",
      "Total T: 409699 Episode Num: 2059 Reward: -5.029890 Avg Reward: -0.4483588saving best model....\n",
      "\n",
      "Total T: 409950 Episode Num: 2060 Reward: 6.661103 Avg Reward: -0.281152saving best model....\n",
      "\n",
      "Total T: 410201 Episode Num: 2061 Reward: -4.591801 Avg Reward: -0.277503saving best model....\n",
      "\n",
      "Total T: 410452 Episode Num: 2062 Reward: 9.364082 Avg Reward: -0.093623saving best model....\n",
      "\n",
      "Total T: 411518 Episode Num: 2067 Reward: -6.224425 Avg Reward: -0.171027saving best model....\n",
      "\n",
      "Total T: 427679 Episode Num: 2136 Reward: -8.212918 Avg Reward: -1.5413795\n",
      "current winratios: [-0.03 -0.02]\n",
      "\n",
      "Total T: 449231 Episode Num: 2236 Reward: 17.724574 Avg Reward: -0.8700010\n",
      "current winratios: [-0.01 -0.09]\n",
      "\n",
      "Total T: 471719 Episode Num: 2336 Reward: 9.423166 Avg Reward: -1.95427732\n",
      "current winratios: [ 0.05 -0.13]\n",
      "\n",
      "Total T: 494425 Episode Num: 2436 Reward: 17.736302 Avg Reward: -0.3480776\n",
      "current winratios: [ 0.01 -0.06]\n",
      "\n",
      "Total T: 516381 Episode Num: 2536 Reward: -14.247062 Avg Reward: -0.944420\n",
      "current winratios: [ 0.02 -0.01]\n",
      "\n",
      "Total T: 524370 Episode Num: 2571 Reward: -5.601737 Avg Reward: -0.0968955saving best model....\n",
      "\n",
      "Total T: 524621 Episode Num: 2572 Reward: 0.740195 Avg Reward: -0.000171saving best model....\n",
      "\n",
      "Total T: 524872 Episode Num: 2573 Reward: -4.833120 Avg Reward: 0.002365saving best model....\n",
      "\n",
      "Total T: 525876 Episode Num: 2577 Reward: -5.179314 Avg Reward: 0.050159saving best model....\n",
      "\n",
      "Total T: 539515 Episode Num: 2636 Reward: 1.573376 Avg Reward: -0.69875944\n",
      "current winratios: [0.04 0.01]\n",
      "\n",
      "Total T: 553559 Episode Num: 2701 Reward: -5.875470 Avg Reward: 0.07090188saving best model....\n",
      "\n",
      "Total T: 560245 Episode Num: 2732 Reward: -0.430932 Avg Reward: 0.11616931saving best model....\n",
      "\n",
      "Total T: 561068 Episode Num: 2736 Reward: -14.072935 Avg Reward: 0.117380\n",
      "current winratios: [0.02 0.  ]\n",
      "\n",
      "Total T: 583476 Episode Num: 2836 Reward: -5.586438 Avg Reward: -2.3306769\n",
      "current winratios: [ 0.01 -0.08]\n",
      "\n",
      "Total T: 604825 Episode Num: 2936 Reward: 2.010540 Avg Reward: -3.51717994\n",
      "current winratios: [ 0.   -0.24]\n",
      "\n",
      "Total T: 626771 Episode Num: 3033 Reward: -5.737743 Avg Reward: 0.26319592saving best model....\n",
      "\n",
      "Total T: 627524 Episode Num: 3036 Reward: -1.849874 Avg Reward: 0.465672\n",
      "current winratios: [ 0.04 -0.12]\n",
      "\n",
      "Total T: 650041 Episode Num: 3136 Reward: 2.540725 Avg Reward: -0.06727065\n",
      "current winratios: [0.08 0.02]\n",
      "\n",
      "Total T: 672487 Episode Num: 3236 Reward: -12.131118 Avg Reward: -0.678908\n",
      "current winratios: [ 0.06 -0.04]\n",
      "\n",
      "Total T: 694651 Episode Num: 3336 Reward: 2.251507 Avg Reward: 0.146192621\n",
      "current winratios: [ 0.1  -0.08]\n",
      "\n",
      "Total T: 703069 Episode Num: 3371 Reward: -4.696283 Avg Reward: 0.2769128saving best model....\n",
      "\n",
      "Total T: 703232 Episode Num: 3372 Reward: 19.899942 Avg Reward: 0.602024saving best model....\n",
      "\n",
      "Total T: 703800 Episode Num: 3375 Reward: -5.509815 Avg Reward: 0.559988saving best model....\n",
      "\n",
      "Total T: 704302 Episode Num: 3377 Reward: -6.144393 Avg Reward: 0.679585saving best model....\n",
      "\n",
      "Total T: 704553 Episode Num: 3378 Reward: 17.880834 Avg Reward: 0.856295saving best model....\n",
      "\n",
      "Total T: 704804 Episode Num: 3379 Reward: -4.960144 Avg Reward: 0.862549saving best model....\n",
      "\n",
      "Total T: 705055 Episode Num: 3380 Reward: 4.640832 Avg Reward: 0.891029saving best model....\n",
      "\n",
      "Total T: 705306 Episode Num: 3381 Reward: -6.133229 Avg Reward: 0.894902saving best model....\n",
      "\n",
      "Total T: 705557 Episode Num: 3382 Reward: 6.814838 Avg Reward: 0.997142saving best model....\n",
      "\n",
      "Total T: 705808 Episode Num: 3383 Reward: -5.992853 Avg Reward: 0.997178saving best model....\n",
      "\n",
      "Total T: 706059 Episode Num: 3384 Reward: 12.825295 Avg Reward: 1.139134saving best model....\n",
      "\n",
      "Total T: 706310 Episode Num: 3385 Reward: -4.003836 Avg Reward: 1.159834saving best model....\n",
      "\n",
      "Total T: 706561 Episode Num: 3386 Reward: 5.285427 Avg Reward: 1.161232saving best model....\n",
      "\n",
      "Total T: 706812 Episode Num: 3387 Reward: -4.533462 Avg Reward: 1.163028saving best model....\n",
      "\n",
      "Total T: 718032 Episode Num: 3436 Reward: -1.899377 Avg Reward: 0.9826592\n",
      "current winratios: [ 0.1  -0.06]\n",
      "\n",
      "Total T: 721741 Episode Num: 3453 Reward: -5.684269 Avg Reward: 1.3608095saving best model....\n",
      "\n",
      "Total T: 723020 Episode Num: 3459 Reward: -4.330832 Avg Reward: 1.452458saving best model....\n",
      "\n",
      "Total T: 723645 Episode Num: 3462 Reward: 5.259491 Avg Reward: 1.5758552saving best model....\n",
      "\n",
      "Total T: 723896 Episode Num: 3463 Reward: 4.985687 Avg Reward: 1.680071saving best model....\n",
      "\n",
      "Total T: 724649 Episode Num: 3466 Reward: 10.468730 Avg Reward: 1.711064saving best model....\n",
      "\n",
      "Total T: 740053 Episode Num: 3536 Reward: -15.383452 Avg Reward: 0.687833\n",
      "current winratios: [0.09 0.01]\n",
      "\n",
      "Total T: 763527 Episode Num: 3636 Reward: 6.606280 Avg Reward: -0.57196969\n",
      "current winratios: [ 0.04 -0.01]\n",
      "\n",
      "Total T: 785580 Episode Num: 3736 Reward: 15.146256 Avg Reward: -0.7420562\n",
      "current winratios: [ 0.03 -0.05]\n",
      "\n",
      "Total T: 808077 Episode Num: 3836 Reward: 0.764362 Avg Reward: -0.23361013\n",
      "current winratios: [ 0.03 -0.08]\n",
      "\n",
      "Total T: 830659 Episode Num: 3936 Reward: -3.286544 Avg Reward: -0.5308243\n",
      "current winratios: [ 0.01 -0.03]\n",
      "\n",
      "Total T: 839844 Episode Num: 3977 Reward: -5.871653 Avg Reward: -0.7601075"
     ]
    }
   ],
   "source": [
    "# Train agent\n",
    "train(policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.save(filename=\"last\", directory=\"TD3/saves\")\n",
    "\n",
    "evaluate_policy(policy, env, eval_episodes=100, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.update_weights(add=True)\n",
    "stats = np.array(env.weights).T\n",
    "x,y = stats.shape\n",
    "plt.figure(figsize=(16,5))\n",
    "for i,agent,name in zip(stats,env.agents,['weak','strong']):\n",
    "    plt.plot(np.arange(y-1)*50,i[1:],label=name)\n",
    "plt.legend()\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Winscore')\n",
    "plt.savefig('TD3/Plots/last.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
